{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Read me\n",
        "## Companion animals, mental health, and well-being during the COVID-19 pandemic\n",
        "\n",
        "This document contains the original surveys, in French and in English, used\n",
        "for this study.\n",
        "\n",
        "The English version has the variable codes used for the original\n",
        "data extraction.\n",
        "\n",
        "The dataset includes the 23 variables used in the published manuscript.\n",
        "* D2: province or territory\n",
        "* region: Canadian geographical regions divided as British Columbia, Prairies\n",
        "and Territories (Alberta, Saskatchewan, Manitoba, Nunavut, Yukon, and Northwest\n",
        "Territories), Ontario, Qu√©bec, and Atlantic (New Brunswick, Nova Scotia,\n",
        "Prince-Edward-Island, and Newfoundland and Labrador)\n",
        "* O1: pet ownership\n",
        "* O8b: change in pet ownership\n",
        "* P1: pet attitude\n",
        "* D1: age\n",
        "* D5: highest level of education\n",
        "* D67: ethnicity\n",
        "* D4: gender\n",
        "* D11: number of people in the household\n",
        "* D8: yearly income\n",
        "* E3: change in income since the beginning of the pandemic\n",
        "* H2: disability\n",
        "* H1Ax: emotional, psychological or mental health condition\n",
        "* H4: change in mental health since the beginning of the pandemic\n",
        "* H56: tested positive to COVID-19 (or household member)\n",
        "* S: social support\n",
        "* QOL: quality of life (EQ-5D-5L score converted on the scale developed for Canada)\n",
        "* GH: self-assessed overall health\n",
        "* L1: loneliness\n",
        "* H3: perceived mental health\n",
        "* Q2: self-reported level of stress\n",
        "* Q1_cat: anxiety\n",
        "\n",
        "The variables O8b, D67, H56, S, QOL, and Q1_cat were obtained from the\n",
        "combination of other variables, which can be obtained by contacting the authors.\n",
        "\n",
        "The R code includes details for descriptive statistics, Bayesian mixed\n",
        "univariable and multivariable ordinal and linear models, diagnostics, and plots.\n",
        "\n",
        "Tables with the estimates of models from stratified data (dog owners-only and\n",
        "cat owners-only) were added on Jan. 31, 2022.\n"
      ],
      "metadata": {
        "id": "KooYa6tmDVKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eQXHg3omALRy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"dataset_petandcovid.csv\", encoding='ISO-8859-1')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATE and Estimations"
      ],
      "metadata": {
        "id": "QbzbZjqUretE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def preprocess_data(data, o8b_treated='New', o8b_control='No'):\n",
        "    print(\"Starting preprocessing...\")\n",
        "    df = data.copy()\n",
        "\n",
        "    # Print initial shape and O8b values\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "    print(\"O8b value counts:\")\n",
        "    print(df['O8b'].value_counts(dropna=False))\n",
        "\n",
        "    # Print available columns\n",
        "    print(\"Available columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Define treatment (T) from O8b\n",
        "    df['treatment'] = np.where(df['O8b'] == o8b_treated, 1,\n",
        "                             np.where(df['O8b'] == o8b_control, 0, np.nan))\n",
        "\n",
        "    # Check treatment creation\n",
        "    print(\"Treatment value counts:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "\n",
        "    # Filter to keep only T = 0 or 1\n",
        "    df = df[df['treatment'].notna()]\n",
        "    print(f\"Shape after filtering treatment: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows remain after filtering O8b. Check o8b_treated and o8b_control values.\")\n",
        "\n",
        "    # Define outcome (Y)\n",
        "    outcome = 'QOL'\n",
        "    if outcome not in df.columns:\n",
        "        raise ValueError(f\"Outcome column '{outcome}' not found in data.\")\n",
        "\n",
        "    # Define confounders (X)\n",
        "    confounders = ['D4', 'D5', 'D67', 'D11', 'H2', 'H1Ax', 'H56', 'P1', 'region']\n",
        "    # Keep only available confounders\n",
        "    confounders = [col for col in confounders if col in df.columns]\n",
        "    print(f\"Available confounders: {confounders}\")\n",
        "    if not confounders:\n",
        "        print(\"Warning: No confounders found. Proceeding with treatment and outcome only.\")\n",
        "\n",
        "    # Print missing values\n",
        "    print(\"Missing values before imputation:\")\n",
        "    print(df[[outcome, 'treatment'] + confounders].isna().sum())\n",
        "\n",
        "    # Print sample data for categorical confounders\n",
        "    print(\"Sample data for categorical confounders:\")\n",
        "    categorical_sample = [c for c in ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56'] if c in df.columns]\n",
        "    if categorical_sample:\n",
        "        print(df[categorical_sample].head())\n",
        "\n",
        "    # Handle categorical variables with one-hot encoding\n",
        "    categorical_vars = ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
        "    categorical_vars = [var for var in categorical_vars if var in confounders]\n",
        "    print(f\"One-hot encoding categorical variables: {categorical_vars}\")\n",
        "    if categorical_vars:\n",
        "        df = pd.get_dummies(df, columns=categorical_vars, drop_first=True, dtype=int)  # Use int dtype\n",
        "\n",
        "    # Update confounder list to include dummy variables\n",
        "    confounder_cols = [col for col in df.columns if any(col.startswith(var + '_') for var in categorical_vars)]\n",
        "    confounder_cols += [var for var in confounders if var not in categorical_vars]\n",
        "    print(f\"Confounder columns after encoding: {confounder_cols}\")\n",
        "\n",
        "    # Print dtypes of dummy variables\n",
        "    print(\"Dtypes of dummy variables:\")\n",
        "    dummy_cols = [col for col in confounder_cols if any(col.startswith(var + '_') for var in categorical_vars)]\n",
        "    if dummy_cols:\n",
        "        print(df[dummy_cols].dtypes)\n",
        "\n",
        "    # Define continuous variables\n",
        "    continuous_vars = ['D11', 'P1']\n",
        "    continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
        "    print(f\"Continuous variables: {continuous_vars}\")\n",
        "\n",
        "    # Convert continuous variables to numeric\n",
        "    for var in continuous_vars:\n",
        "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "    # Check for valid continuous variables (non-empty after conversion)\n",
        "    valid_continuous_vars = [var for var in continuous_vars if df[var].notna().any()]\n",
        "    print(f\"Valid continuous variables (non-empty): {valid_continuous_vars}\")\n",
        "\n",
        "    # Impute and standardize continuous variables\n",
        "    if valid_continuous_vars:\n",
        "        print(f\"Imputing and standardizing: {valid_continuous_vars}\")\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        imputed_data = imputer.fit_transform(df[valid_continuous_vars])\n",
        "        if imputed_data.shape[1] != len(valid_continuous_vars):\n",
        "            raise ValueError(f\"Imputation output has {imputed_data.shape[1]} columns, expected {len(valid_continuous_vars)}\")\n",
        "        df[valid_continuous_vars] = imputed_data\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = scaler.fit_transform(df[valid_continuous_vars])\n",
        "        df[valid_continuous_vars] = scaled_data\n",
        "    else:\n",
        "        print(\"No valid continuous variables for standardization.\")\n",
        "\n",
        "    # Impute categorical confounders only if missing values exist\n",
        "    categorical_confounders = [col for col in confounder_cols if col not in continuous_vars]\n",
        "    if categorical_confounders:\n",
        "        missing_cat = df[categorical_confounders].isna().any().any()\n",
        "        print(f\"Missing values in categorical confounders: {missing_cat}\")\n",
        "        if missing_cat:\n",
        "            print(f\"Imputing categorical confounders: {categorical_confounders}\")\n",
        "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "            imputed_cat_data = imputer_cat.fit_transform(df[categorical_confounders])\n",
        "            if imputed_cat_data.shape[1] != len(categorical_confounders):\n",
        "                raise ValueError(f\"Categorical imputation output has {imputed_cat_data.shape[1]} columns, expected {len(categorical_confounders)}\")\n",
        "            df[categorical_confounders] = imputed_cat_data\n",
        "        else:\n",
        "            print(\"No imputation needed for categorical confounders (no missing values).\")\n",
        "\n",
        "    # Impute outcome if missing\n",
        "    if df[outcome].isna().any():\n",
        "        print(\"Imputing missing QOL values...\")\n",
        "        imputer_outcome = SimpleImputer(strategy='mean')\n",
        "        df[outcome] = imputer_outcome.fit_transform(df[[outcome]])\n",
        "\n",
        "    # Check data types of confounders\n",
        "    print(\"Data types of confounder columns:\")\n",
        "    print(df[confounder_cols].dtypes)\n",
        "\n",
        "    # Ensure all confounders are numeric\n",
        "    non_numeric_cols = df[confounder_cols].select_dtypes(exclude=['int64', 'float64', 'uint8', 'int32']).columns\n",
        "    if non_numeric_cols.any():\n",
        "        raise ValueError(f\"Non-numeric confounders detected: {non_numeric_cols.tolist()}\")\n",
        "\n",
        "    # Final check for missing values\n",
        "    print(\"Missing values after imputation:\")\n",
        "    final_cols = [outcome, 'treatment'] + (confounder_cols if confounder_cols else [])\n",
        "    print(df[final_cols].isna().sum())\n",
        "\n",
        "    print(f\"Final shape after preprocessing: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No data remains after preprocessing.\")\n",
        "\n",
        "    return df, outcome, confounder_cols\n",
        "\n",
        "# Step 2: AIPTW Estimator\n",
        "def estimate_aiptw(df, outcome, confounders):\n",
        "    T = df['treatment'].values\n",
        "    Y = df[outcome].values\n",
        "    X = df[confounders].values if confounders else np.ones((len(df), 1))  # Fallback for no confounders\n",
        "\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(X, T)\n",
        "    ps = ps_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    treated_idx = T == 1\n",
        "    control_idx = T == 0\n",
        "\n",
        "    outcome_model_treated = LinearRegression().fit(X[treated_idx], Y[treated_idx])\n",
        "    outcome_model_control = LinearRegression().fit(X[control_idx], Y[control_idx])\n",
        "\n",
        "    mu1_hat = outcome_model_treated.predict(X)\n",
        "    mu0_hat = outcome_model_control.predict(X)\n",
        "\n",
        "    term1 = (T * Y / ps) + (1 - T / ps) * mu1_hat\n",
        "    term0 = ((1 - T) * Y / (1 - ps)) + (1 - (1 - T) / (1 - ps)) * mu0_hat\n",
        "\n",
        "    mu1 = np.mean(term1)\n",
        "    mu0 = np.mean(term0)\n",
        "    ate = mu1 - mu0\n",
        "\n",
        "    n = len(Y)\n",
        "    boot_ates = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        T_boot = T[idx]\n",
        "        Y_boot = Y[idx]\n",
        "        X_boot = X[idx]\n",
        "        ps_boot = ps_model.predict_proba(X_boot)[:, 1]\n",
        "        mu1_boot = outcome_model_treated.predict(X_boot)\n",
        "        mu0_boot = outcome_model_control.predict(X_boot)\n",
        "        term1_boot = (T_boot * Y_boot / ps_boot) + (1 - T_boot / ps_boot) * mu1_boot\n",
        "        term0_boot = ((1 - T_boot) * Y_boot / (1 - ps_boot)) + (1 - (1 - T_boot) / (1 - ps_boot)) * mu0_boot\n",
        "        boot_ates.append(np.mean(term1_boot) - np.mean(term0_boot))\n",
        "\n",
        "    se = np.std(boot_ates)\n",
        "    ci = (ate - 1.96 * se, ate + 1.96 * se)\n",
        "\n",
        "    return ate, se, ci, ps\n",
        "\n",
        "# Step 3: IPTW Estimator\n",
        "def estimate_iptw(df, outcome, confounders):\n",
        "    T = df['treatment'].values\n",
        "    Y = df[outcome].values\n",
        "    X = df[confounders].values if confounders else np.ones((len(df), 1))\n",
        "\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(X, T)\n",
        "    ps = ps_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    weights = T / ps + (1 - T) / (1 - ps)\n",
        "    weights = np.clip(weights, 0.1, 10)\n",
        "\n",
        "    mu1 = np.sum((T * Y * weights)) / np.sum(T * weights)\n",
        "    mu0 = np.sum(((1 - T) * Y * weights)) / np.sum((1 - T) * weights)\n",
        "    ate = mu1 - mu0\n",
        "\n",
        "    df['weight'] = weights\n",
        "    model = sm.WLS(Y, sm.add_constant(T), weights=weights).fit(cov_type='HC3')\n",
        "    se = model.bse[1]\n",
        "    ci = (ate - 1.96 * se, ate + 1.96 * se)\n",
        "\n",
        "    return ate, se, ci, ps\n",
        "\n",
        "# Step 4: Regression Adjustment\n",
        "def estimate_regression(df, outcome, confounders):\n",
        "    X = df[['treatment'] + confounders].copy() if confounders else df[['treatment']].copy()\n",
        "    X = sm.add_constant(X)\n",
        "    Y = df[outcome]\n",
        "\n",
        "    model = sm.OLS(Y, X).fit(cov_type='HC3')\n",
        "    ate = model.params['treatment']\n",
        "    se = model.bse['treatment']\n",
        "    ci = (ate - 1.96 * se, ate + 1.96 * se)\n",
        "\n",
        "    return ate, se, ci\n",
        "\n",
        "# Step 5: Diagnostics\n",
        "def plot_diagnostics(ps, df, output_dir='plots'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(ps[df['treatment'] == 1], label='Treated', color='blue', alpha=0.5)\n",
        "    sns.histplot(ps[df['treatment'] == 0], label='Control', color='orange', alpha=0.5)\n",
        "    plt.title('Propensity Score Distribution')\n",
        "    plt.xlabel('Propensity Score')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{output_dir}/ps_overlap.png')\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(df['weight'], color='green', alpha=0.5)\n",
        "    plt.title('IPTW Weight Distribution')\n",
        "    plt.xlabel('Weight')\n",
        "    plt.savefig(f'{output_dir}/weight_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "# Main function\n",
        "def main(data, o8b_treated='New', o8b_control='No'):\n",
        "    df, outcome, confounders = preprocess_data(data, o8b_treated, o8b_control)\n",
        "\n",
        "    print(\"Estimating ATE...\")\n",
        "\n",
        "    ate_aiptw, se_aiptw, ci_aiptw, ps = estimate_aiptw(df, outcome, confounders)\n",
        "    print(f\"AIPTW: ATE = {ate_aiptw:.3f}, SE = {se_aiptw:.3f}, 95% CI = ({ci_aiptw[0]:.3f}, {ci_aiptw[1]:.3f})\")\n",
        "\n",
        "    ate_iptw, se_iptw, ci_iptw, _ = estimate_iptw(df, outcome, confounders)\n",
        "    print(f\"IPTW: ATE = {ate_iptw:.3f}, SE = {se_iptw:.3f}, 95% CI = ({ci_iptw[0]:.3f}, {ci_iptw[1]:.3f})\")\n",
        "\n",
        "    ate_reg, se_reg, ci_reg = estimate_regression(df, outcome, confounders)\n",
        "    print(f\"Regression: ATE = {ate_reg:.3f}, SE = {se_reg:.3f}, 95% CI = ({ci_reg[0]:.3f}, {ci_reg[1]:.3f})\")\n",
        "\n",
        "    plot_diagnostics(ps, df)\n",
        "    print(\"Diagnostic plots saved in 'plots' directory.\")\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    main(data, o8b_treated='New', o8b_control='No')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nlbBYLLCV7V",
        "outputId": "2fa95400-2a33-424d-8a09-c29549522d43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing...\n",
            "Initial shape: (1500, 23)\n",
            "O8b value counts:\n",
            "O8b\n",
            "No      1259\n",
            "Loss     173\n",
            "New       68\n",
            "Name: count, dtype: int64\n",
            "Available columns:\n",
            "['D2', 'region', 'O1', 'O8b', 'P1', 'D1', 'D5', 'D67', 'D4', 'D11', 'D8', 'E3', 'H2', 'H1Ax', 'H4', 'H56', 'S', 'QOL', 'GH', 'L1', 'H3', 'Q2', 'Q1_cat']\n",
            "Treatment value counts:\n",
            "treatment\n",
            "0.0    1259\n",
            "NaN     173\n",
            "1.0      68\n",
            "Name: count, dtype: int64\n",
            "Shape after filtering treatment: (1327, 24)\n",
            "Available confounders: ['D4', 'D5', 'D67', 'D11', 'H2', 'H1Ax', 'H56', 'P1', 'region']\n",
            "Missing values before imputation:\n",
            "QOL          0\n",
            "treatment    0\n",
            "D4           0\n",
            "D5           0\n",
            "D67          0\n",
            "D11          0\n",
            "H2           0\n",
            "H1Ax         0\n",
            "H56          0\n",
            "P1           0\n",
            "region       0\n",
            "dtype: int64\n",
            "Sample data for categorical confounders:\n",
            "      D4           D5         D67    region   H2 H1Ax H56\n",
            "0    Men  High school      Others  Atlantic   No   No  No\n",
            "1    Men      College  Caucasians  Prairies   No   No  No\n",
            "2    Men   University  Caucasians  Atlantic  Yes  Yes  No\n",
            "3    Men  High school  Caucasians   Ontario   No   No  No\n",
            "4  Women   University  Caucasians   Ontario   No   No  No\n",
            "One-hot encoding categorical variables: ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
            "Confounder columns after encoding: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11', 'P1']\n",
            "Dtypes of dummy variables:\n",
            "D4_Other or did not answer    int64\n",
            "D4_Women                      int64\n",
            "D5_College                    int64\n",
            "D5_High school                int64\n",
            "D5_University                 int64\n",
            "D67_Others                    int64\n",
            "region_British Columbia       int64\n",
            "region_Ontario                int64\n",
            "region_Prairies               int64\n",
            "region_Qu√©bec                 int64\n",
            "H2_Yes                        int64\n",
            "H1Ax_Yes                      int64\n",
            "H56_Yes                       int64\n",
            "dtype: object\n",
            "Continuous variables: ['D11', 'P1']\n",
            "Valid continuous variables (non-empty): ['D11', 'P1']\n",
            "Imputing and standardizing: ['D11', 'P1']\n",
            "Missing values in categorical confounders: False\n",
            "No imputation needed for categorical confounders (no missing values).\n",
            "Data types of confounder columns:\n",
            "D4_Other or did not answer      int64\n",
            "D4_Women                        int64\n",
            "D5_College                      int64\n",
            "D5_High school                  int64\n",
            "D5_University                   int64\n",
            "D67_Others                      int64\n",
            "region_British Columbia         int64\n",
            "region_Ontario                  int64\n",
            "region_Prairies                 int64\n",
            "region_Qu√©bec                   int64\n",
            "H2_Yes                          int64\n",
            "H1Ax_Yes                        int64\n",
            "H56_Yes                         int64\n",
            "D11                           float64\n",
            "P1                            float64\n",
            "dtype: object\n",
            "Missing values after imputation:\n",
            "QOL                           0\n",
            "treatment                     0\n",
            "D4_Other or did not answer    0\n",
            "D4_Women                      0\n",
            "D5_College                    0\n",
            "D5_High school                0\n",
            "D5_University                 0\n",
            "D67_Others                    0\n",
            "region_British Columbia       0\n",
            "region_Ontario                0\n",
            "region_Prairies               0\n",
            "region_Qu√©bec                 0\n",
            "H2_Yes                        0\n",
            "H1Ax_Yes                      0\n",
            "H56_Yes                       0\n",
            "D11                           0\n",
            "P1                            0\n",
            "dtype: int64\n",
            "Final shape after preprocessing: (1327, 30)\n",
            "Estimating ATE...\n",
            "AIPTW: ATE = -0.008, SE = 0.011, 95% CI = (-0.030, 0.014)\n",
            "IPTW: ATE = -0.013, SE = 0.018, 95% CI = (-0.049, 0.023)\n",
            "Regression: ATE = -0.008, SE = 0.016, 95% CI = (-0.039, 0.023)\n",
            "Diagnostic plots saved in 'plots' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching, ATT, and Sensitivity Analysis"
      ],
      "metadata": {
        "id": "kkv7KwJdrYTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import r2_score\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def preprocess_data(data, o8b_treated='New', o8b_control='No'):\n",
        "    print(\"Starting preprocessing...\")\n",
        "    df = data.copy()\n",
        "\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "    print(\"O8b value counts:\")\n",
        "    print(df['O8b'].value_counts(dropna=False))\n",
        "\n",
        "    print(\"Available columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    df['treatment'] = np.where(df['O8b'] == o8b_treated, 1,\n",
        "                             np.where(df['O8b'] == o8b_control, 0, np.nan))\n",
        "\n",
        "    print(\"Treatment value counts:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "\n",
        "    df = df[df['treatment'].notna()]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(f\"Shape after filtering treatment: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows remain after filtering O8b. Check o8b_treated and o8b_control values.\")\n",
        "\n",
        "    outcome = 'QOL'\n",
        "    if outcome not in df.columns:\n",
        "        raise ValueError(f\"Outcome column '{outcome}' not found in data.\")\n",
        "\n",
        "    confounders = ['D4', 'D5', 'D67', 'D11', 'H2', 'H1Ax', 'H56', 'P1', 'region']\n",
        "    confounders = [col for col in confounders if col in df.columns]\n",
        "    print(f\"Available confounders: {confounders}\")\n",
        "\n",
        "    print(\"Missing values before imputation:\")\n",
        "    print(df[[outcome, 'treatment'] + confounders].isna().sum())\n",
        "\n",
        "    print(\"Sample data for categorical confounders:\")\n",
        "    categorical_sample = [c for c in ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56'] if c in df.columns]\n",
        "    if categorical_sample:\n",
        "        print(df[categorical_sample].head())\n",
        "\n",
        "    categorical_vars = ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
        "    categorical_vars = [var for var in categorical_vars if var in confounders]\n",
        "    print(f\"One-hot encoding categorical variables: {categorical_vars}\")\n",
        "    if categorical_vars:\n",
        "        df = pd.get_dummies(df, columns=categorical_vars, drop_first=True, dtype=int)\n",
        "\n",
        "    confounder_cols = [col for col in df.columns if any(col.startswith(var + '_') for var in categorical_vars)]\n",
        "    confounder_cols += [var for var in confounders if var not in categorical_vars]\n",
        "    print(f\"Confounder columns after encoding: {confounder_cols}\")\n",
        "\n",
        "    continuous_vars = ['D11', 'P1']\n",
        "    continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
        "    print(f\"Continuous variables: {continuous_vars}\")\n",
        "\n",
        "    for var in continuous_vars:\n",
        "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "    valid_continuous_vars = [var for var in continuous_vars if df[var].notna().any()]\n",
        "    print(f\"-utils continuous variables (non-empty): {valid_continuous_vars}\")\n",
        "\n",
        "    if valid_continuous_vars:\n",
        "        print(f\"Imputing and standardizing: {valid_continuous_vars}\")\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        df[valid_continuous_vars] = imputer.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        df[valid_continuous_vars] = scaler.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "    categorical_confounders = [col for col in confounder_cols if col not in continuous_vars]\n",
        "    if categorical_confounders:\n",
        "        missing_cat = df[categorical_confounders].isna().any().any()\n",
        "        print(f\"Missing values in categorical confounders: {missing_cat}\")\n",
        "        if missing_cat:\n",
        "            print(f\"Imputing categorical confounders: {categorical_confounders}\")\n",
        "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "            df[categorical_confounders] = imputer_cat.fit_transform(df[categorical_confounders])\n",
        "\n",
        "    if df[outcome].isna().any():\n",
        "        print(\"Imputing missing QOL values...\")\n",
        "        imputer_outcome = SimpleImputer(strategy='mean')\n",
        "        df[outcome] = imputer_outcome.fit_transform(df[[outcome]])\n",
        "\n",
        "    print(\"Data types of confounder columns:\")\n",
        "    print(df[confounder_cols].dtypes)\n",
        "\n",
        "    non_numeric_cols = df[confounder_cols].select_dtypes(exclude=['int64', 'float64', 'uint8', 'int32']).columns\n",
        "    if non_numeric_cols.any():\n",
        "        raise ValueError(f\"Non-numeric confounders detected: {non_numeric_cols.tolist()}\")\n",
        "\n",
        "    print(\"Missing values after imputation:\")\n",
        "    final_cols = [outcome, 'treatment'] + (confounder_cols if confounder_cols else [])\n",
        "    print(df[final_cols].isna().sum())\n",
        "\n",
        "    print(f\"Final shape after preprocessing: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No data remains after preprocessing.\")\n",
        "\n",
        "    return df, outcome, confounder_cols\n",
        "\n",
        "# Step 2: Propensity Score Matching\n",
        "def propensity_score_matching(df, confounders):\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(df[confounders], df['treatment'])\n",
        "    ps = ps_model.predict_proba(df[confounders])[:, 1]\n",
        "\n",
        "    treated_idx = df[df['treatment'] == 1].index\n",
        "    control_idx = df[df['treatment'] == 0].index\n",
        "\n",
        "    treated_ps = ps[treated_idx]\n",
        "    control_ps = ps[control_idx]\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=1)\n",
        "    nn.fit(control_ps.reshape(-1, 1))\n",
        "    distances, indices = nn.kneighbors(treated_ps.reshape(-1, 1))\n",
        "\n",
        "    matched_control_idx = control_idx[indices.flatten()]\n",
        "    matched_df = df.loc[list(treated_idx) + list(matched_control_idx)]\n",
        "\n",
        "    print(f\"Shape of matched dataset: {matched_df.shape}\")\n",
        "    return matched_df, ps\n",
        "\n",
        "# Step 3: IPTW for ATT\n",
        "def estimate_iptw_att(df, outcome, confounders):\n",
        "    T = df['treatment'].values\n",
        "    Y = df[outcome].values\n",
        "    X = df[confounders].values if confounders else np.ones((len(df), 1))\n",
        "\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(X, T)\n",
        "    ps = ps_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    weights = np.where(T == 1, 1, ps / (1 - ps))\n",
        "    weights = np.clip(weights, 0.1, 10)\n",
        "\n",
        "    treated_idx = T == 1\n",
        "    control_idx = T == 0\n",
        "\n",
        "    mu1 = np.mean(Y[treated_idx])\n",
        "    mu0 = np.average(Y[control_idx], weights=weights[control_idx])\n",
        "    att = mu1 - mu0\n",
        "\n",
        "    n = len(Y)\n",
        "    boot_atts = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        T_boot = T[idx]\n",
        "        Y_boot = Y[idx]\n",
        "        weights_boot = weights[idx]\n",
        "        treated_boot = T_boot == 1\n",
        "        control_boot = T_boot == 0\n",
        "        mu1_boot = np.mean(Y_boot[treated_boot])\n",
        "        mu0_boot = np.average(Y_boot[control_boot], weights=weights_boot[control_boot])\n",
        "        boot_atts.append(mu1_boot - mu0_boot)\n",
        "\n",
        "    se = np.std(boot_atts)\n",
        "    ci = (att - 1.96 * se, att + 1.96 * se)\n",
        "\n",
        "    df['weight'] = weights\n",
        "    return att, se, ci, ps\n",
        "\n",
        "# Step 4: AIPTW (for Matched Data)\n",
        "def estimate_aiptw(df, outcome, confounders):\n",
        "    T = df['treatment'].values\n",
        "    Y = df[outcome].values\n",
        "    X = df[confounders].values if confounders else np.ones((len(df), 1))\n",
        "\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(X, T)\n",
        "    ps = ps_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    treated_idx = T == 1\n",
        "    control_idx = T == 0\n",
        "\n",
        "    outcome_model_treated = LinearRegression().fit(X[treated_idx], Y[treated_idx])\n",
        "    outcome_model_control = LinearRegression().fit(X[control_idx], Y[control_idx])\n",
        "\n",
        "    mu1_hat = outcome_model_treated.predict(X)\n",
        "    mu0_hat = outcome_model_control.predict(X)\n",
        "\n",
        "    term1 = (T * Y / ps) + (1 - T / ps) * mu1_hat\n",
        "    term0 = ((1 - T) * Y / (1 - ps)) + (1 - (1 - T) / (1 - ps)) * mu0_hat\n",
        "\n",
        "    mu1 = np.mean(term1)\n",
        "    mu0 = np.mean(term0)\n",
        "    ate = mu1 - mu0\n",
        "\n",
        "    n = len(Y)\n",
        "    boot_ates = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        T_boot = T[idx]\n",
        "        Y_boot = Y[idx]\n",
        "        X_boot = X[idx]\n",
        "        ps_boot = ps_model.predict_proba(X_boot)[:, 1]\n",
        "        mu1_boot = outcome_model_treated.predict(X_boot)\n",
        "        mu0_boot = outcome_model_control.predict(X_boot)\n",
        "        term1_boot = (T_boot * Y_boot / ps_boot) + (1 - T_boot / ps_boot) * mu1_boot\n",
        "        term0_boot = ((1 - T_boot) * Y_boot / (1 - ps_boot)) + (1 - (1 - T_boot) / (1 - ps_boot)) * mu0_boot\n",
        "        boot_ates.append(np.mean(term1_boot) - np.mean(term0_boot))\n",
        "\n",
        "    se = np.std(boot_ates)\n",
        "    ci = (ate - 1.96 * se, ate + 1.96 * se)\n",
        "\n",
        "    return ate, se, ci, ps\n",
        "\n",
        "# Step 5: Matching for ATT\n",
        "def matching_att(df, outcome, ps):\n",
        "    treated_idx = df[df['treatment'] == 1].index\n",
        "    control_idx = df[df['treatment'] == 0].index\n",
        "\n",
        "    treated_ps = ps[treated_idx]\n",
        "    control_ps = ps[control_idx]\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=1)\n",
        "    nn.fit(control_ps.reshape(-1, 1))\n",
        "    distances, indices = nn.kneighbors(treated_ps.reshape(-1, 1))\n",
        "\n",
        "    matched_control_idx = control_idx[indices.flatten()]\n",
        "    matched_df = df.loc[list(treated_idx) + list(matched_control_idx)]\n",
        "\n",
        "    att = matched_df[matched_df['treatment'] == 1][outcome].mean() - matched_df[matched_df['treatment'] == 0][outcome].mean()\n",
        "\n",
        "    boot_atts = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(len(matched_df), len(matched_df), replace=True)\n",
        "        boot_df = matched_df.iloc[idx]\n",
        "        att_boot = boot_df[boot_df['treatment'] == 1][outcome].mean() - boot_df[boot_df['treatment'] == 0][outcome].mean()\n",
        "        boot_atts.append(att_boot)\n",
        "\n",
        "    se = np.std(boot_atts)\n",
        "    ci = (att - 1.96 * se, att + 1.96 * se)\n",
        "\n",
        "    return att, se, ci\n",
        "\n",
        "# Step 6: Stratified Analysis by Pet Type\n",
        "def stratified_analysis(df, outcome, confounders):\n",
        "    print(\"O1 value counts:\")\n",
        "    print(df['O1'].value_counts(dropna=False))\n",
        "\n",
        "    pet_types = ['Dog', 'Cat']\n",
        "    results = {}\n",
        "\n",
        "    for pet_type in pet_types:\n",
        "        print(f\"\\nStratified analysis for {pet_type} owners:\")\n",
        "        sub_df = df[df['O1'].str.contains(pet_type, case=False, na=False)]\n",
        "        if len(sub_df) < 10 or sub_df['treatment'].nunique() < 2:\n",
        "            print(f\"Skipping {pet_type}: insufficient data (n={len(sub_df)})\")\n",
        "            continue\n",
        "\n",
        "        att, se, ci, ps = estimate_iptw_att(sub_df, outcome, confounders)\n",
        "        results[pet_type] = {'ATT': att, 'SE': se, 'CI': ci}\n",
        "        print(f\"{pet_type} ATT = {att:.3f}, SE = {se:.3f}, 95% CI = ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
        "\n",
        "    if not results:\n",
        "        print(\"\\nTrying binary O1 (pet owners vs. non-owners)...\")\n",
        "        pet_owners = df[df['O1'].str.contains('Yes', case=False, na=False)]\n",
        "        if len(pet_owners) >= 10 and pet_owners['treatment'].nunique() == 2:\n",
        "            att, se, ci, ps = estimate_iptw_att(pet_owners, outcome, confounders)\n",
        "            results['Pet Owners'] = {'ATT': att, 'SE': se, 'CI': ci}\n",
        "            print(f\"Pet Owners ATT = {att:.3f}, SE = {se:.3f}, 95% CI = ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Step 7: Sensitivity Analysis (E-value)\n",
        "def compute_e_value(ate, se):\n",
        "    point_est = abs(ate)\n",
        "    rr_point = np.exp(0.91 * point_est)\n",
        "    e_value_point = rr_point + np.sqrt(rr_point * (rr_point - 1))\n",
        "\n",
        "    lower_bound = abs(ate - 1.96 * se)\n",
        "    if lower_bound > 0:\n",
        "        rr_lower = np.exp(0.91 * lower_bound)\n",
        "        e_value_lower = rr_lower + np.sqrt(rr_lower * (rr_lower - 1))\n",
        "    else:\n",
        "        e_value_lower = 1.0\n",
        "\n",
        "    return e_value_point, e_value_lower\n",
        "\n",
        "# Step 8: Calculate R^2 for Observed Confounders\n",
        "def calculate_r2(df, confounder_cols, target):\n",
        "    r2_values = []\n",
        "    for col in confounder_cols:\n",
        "        X = df[[col]].dropna()\n",
        "        y = df.loc[X.index, target]\n",
        "        if len(X) == 0 or y.isna().all():\n",
        "            continue\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        r2_values.append((col, r2))\n",
        "    return r2_values\n",
        "\n",
        "# Step 9: Austin Plot with Bias Curve\n",
        "def austin_plot_sensitivity(df, outcome, confounder_cols, att_orig, se_orig, output_dir='plots', prefix=''):\n",
        "    print(f\"\\nGenerating Austin plot for sensitivity analysis ({prefix})...\")\n",
        "\n",
        "    T = df['treatment'].values\n",
        "    Y = df[outcome].values\n",
        "\n",
        "    # Compute propensity scores\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(df[confounder_cols], T)\n",
        "    p = ps_model.predict_proba(df[confounder_cols])[:, 1]\n",
        "\n",
        "    # Generate U ~ N(0,1)\n",
        "    U = np.random.normal(0, 1, len(df))\n",
        "\n",
        "    # Define R^2 grid\n",
        "    r2_values = np.linspace(0.0, 0.5, 21)\n",
        "    adjusted_atts = np.zeros((len(r2_values), len(r2_values)))\n",
        "\n",
        "    sd_y = np.std(Y)\n",
        "\n",
        "    for i, r2_ut in enumerate(r2_values):\n",
        "        for j, r2_uy in enumerate(r2_values):\n",
        "            # Simulate U correlated with propensity score\n",
        "            a = np.sqrt(r2_ut)\n",
        "            b = np.sqrt(1 - r2_ut)\n",
        "            p_std = (p - np.mean(p)) / np.std(p)\n",
        "            U_sim = a * p_std + b * U\n",
        "\n",
        "            # Adjust outcome for confounding\n",
        "            gamma = np.sqrt(r2_uy) * sd_y\n",
        "            Y_adj = Y - gamma * U_sim\n",
        "\n",
        "            # Estimate ATT on adjusted outcome\n",
        "            treated_idx = T == 1\n",
        "            control_idx = T == 0\n",
        "            weights = df['weight'].values\n",
        "            mu1 = np.mean(Y_adj[treated_idx])\n",
        "            mu0 = np.average(Y_adj[control_idx], weights=weights[control_idx])\n",
        "            adjusted_atts[i, j] = mu1 - mu0\n",
        "\n",
        "    # Compute R^2 for observed confounders\n",
        "    r2_t = calculate_r2(df, confounder_cols, 'treatment')\n",
        "    r2_y = calculate_r2(df, confounder_cols, outcome)\n",
        "    confounder_r2 = {conf_t: (r2_ut, r2_uy) for (conf_t, r2_ut), (conf_y, r2_uy) in zip(r2_t, r2_y) if conf_t == conf_y}\n",
        "\n",
        "    # Colors and markers\n",
        "    colors = list(mcolors.TABLEAU_COLORS.values())[:len(confounder_r2)]\n",
        "    if len(colors) < len(confounder_r2):\n",
        "        colors.extend(list(mcolors.CSS4_COLORS.values())[:len(confounder_r2) - len(colors)])\n",
        "    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', 'h', '*']\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    contour = plt.contourf(r2_values, r2_values, adjusted_atts, levels=20, cmap='viridis')\n",
        "    plt.colorbar(contour, label='Adjusted ATT', pad=0.1)\n",
        "\n",
        "    # Contour for ATT = 0\n",
        "    plt.contour(r2_values, r2_values, adjusted_atts, levels=[0], colors='red', linestyles='dashed', linewidths=2)\n",
        "\n",
        "    # Bias curve (ATT = lower CI)\n",
        "    ci_lower = att_orig - 1.96 * se_orig\n",
        "    cs_ci = plt.contour(r2_values, r2_values, adjusted_atts, levels=[ci_lower], colors='cyan', linestyles='solid', linewidths=2)\n",
        "    plt.clabel(cs_ci, inline=True, fontsize=10, fmt=f'ATT = {ci_lower:.3f} (Lower CI)')\n",
        "\n",
        "    # Plot observed confounders\n",
        "    for idx, (conf, (r2_ut, r2_uy)) in enumerate(confounder_r2.items()):\n",
        "        plt.scatter(r2_ut, r2_uy, label=conf, color=colors[idx % len(colors)],\n",
        "                    marker=markers[idx % len(markers)], s=150, alpha=0.8, edgecolors='black')\n",
        "\n",
        "    # Annotations\n",
        "    plt.text(0.02, 0.48, f'Original ATT: {att_orig:.3f}', color='white', fontsize=12, fontweight='bold',\n",
        "             bbox=dict(facecolor='black', alpha=0.5))\n",
        "    plt.text(0.02, 0.44, f'CI: ({ci_lower:.3f}, {att_orig + 1.96 * se_orig:.3f})', color='white', fontsize=12, fontweight='bold',\n",
        "             bbox=dict(facecolor='black', alpha=0.5))\n",
        "\n",
        "    plt.xlabel('$R^2_{UT}$ (Influence on Treatment)', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('$R^2_{UY}$ (Influence on Outcome)', fontsize=14, fontweight='bold')\n",
        "    plt.title('Austin Plot for Sensitivity to Unmeasured Confounding', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    num_confounders = len(confounder_r2)\n",
        "    num_cols = min(3, max(1, num_confounders // 4 + 1))\n",
        "    plt.legend(title=\"Observed Confounders\", fontsize=10, title_fontsize=12,\n",
        "               loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=num_cols)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_dir}/austin_plot_{prefix}.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Step 10: Diagnostics with SMD\n",
        "def compute_smd(df, confounders, weights):\n",
        "    treated = df[df['treatment'] == 1]\n",
        "    control = df[df['treatment'] == 0]\n",
        "    weights_t = weights[df['treatment'] == 1]\n",
        "    weights_c = weights[df['treatment'] == 0]\n",
        "    smd_results = {}\n",
        "\n",
        "    for col in confounders:\n",
        "        mean_t = np.average(treated[col], weights=weights_t)\n",
        "        mean_c = np.average(control[col], weights=weights_c)\n",
        "        var_t = np.average((treated[col] - mean_t)**2, weights=weights_t)\n",
        "        var_c = np.average((control[col] - mean_c)**2, weights=weights_c)\n",
        "        smd = (mean_t - mean_c) / np.sqrt((var_t + var_c) / 2)\n",
        "        smd_results[col] = smd\n",
        "\n",
        "    return smd_results\n",
        "\n",
        "def plot_diagnostics(ps, df, output_dir='plots', prefix=''):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(ps[df['treatment'] == 1], label='Treated', color='blue', alpha=0.5)\n",
        "    sns.histplot(ps[df['treatment'] == 0], label='Control', color='orange', alpha=0.5)\n",
        "    plt.title(f'Propensity Score Distribution ({prefix})')\n",
        "    plt.xlabel('Propensity Score')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{output_dir}/{prefix}_ps_overlap.png')\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(df['weight'], color='green', alpha=0.5)\n",
        "    plt.title(f'IPTW Weight Distribution ({prefix})')\n",
        "    plt.xlabel('Weight')\n",
        "    plt.savefig(f'{output_dir}/{prefix}_weight_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "# Main Function\n",
        "def main(data, o8b_treated='New', o8b_control='No'):\n",
        "    df, outcome, confounders = preprocess_data(data, o8b_treated, o8b_control)\n",
        "\n",
        "    print(\"\\nEstimating ATT with IPTW...\")\n",
        "    att_iptw, se_iptw, ci_iptw, ps = estimate_iptw_att(df, outcome, confounders)\n",
        "    print(f\"IPTW ATT = {att_iptw:.3f}, SE = {se_iptw:.3f}, 95% CI = ({ci_iptw[0]:.3f}, {ci_iptw[1]:.3f})\")\n",
        "\n",
        "    print(\"\\nPerforming propensity score matching...\")\n",
        "    matched_df, ps_matched = propensity_score_matching(df, confounders)\n",
        "    ate_matched, se_matched, ci_matched, ps_matched_new = estimate_aiptw(matched_df, outcome, confounders)\n",
        "    print(f\"Matched ATE (AIPTW) = {ate_matched:.3f}, SE = {se_matched:.3f}, 95% CI = ({ci_matched[0]:.3f}, {ci_matched[1]:.3f})\")\n",
        "\n",
        "    att_matched, se_matched_att, ci_matched_att = matching_att(df, outcome, ps)\n",
        "    print(f\"Matched ATT = {att_matched:.3f}, SE = {se_matched_att:.3f}, 95% CI = ({ci_matched_att[0]:.3f}, {ci_matched_att[1]:.3f})\")\n",
        "\n",
        "    print(\"\\nPerforming stratified analysis...\")\n",
        "    stratified_results = stratified_analysis(df, outcome, confounders)\n",
        "\n",
        "    print(\"\\nComputing E-value for IPTW ATT...\")\n",
        "    e_value_point, e_value_lower = compute_e_value(att_iptw, se_iptw)\n",
        "    print(f\"E-value (point estimate) = {e_value_point:.2f}\")\n",
        "    print(f\"E-value (lower CI bound) = {e_value_lower:.2f}\")\n",
        "\n",
        "    # Add Austin plot\n",
        "    austin_plot_sensitivity(df, outcome, confounders, att_iptw, se_iptw, prefix='iptw_att')\n",
        "\n",
        "    print(\"\\nComputing SMD for IPTW ATT...\")\n",
        "    smd_results = compute_smd(df, confounders, df['weight'])\n",
        "    for col, smd in smd_results.items():\n",
        "        print(f\"SMD for {col}: {smd:.3f}\")\n",
        "\n",
        "    plot_diagnostics(ps, df, prefix='full')\n",
        "    plot_diagnostics(ps_matched_new, matched_df, prefix='matched')\n",
        "    print(\"Diagnostic plots saved in 'plots' directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(data, o8b_treated='New', o8b_control='No')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daout_65JVHD",
        "outputId": "e2f05553-3f22-49c0-ab9b-591667fef290"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing...\n",
            "Initial shape: (1500, 23)\n",
            "O8b value counts:\n",
            "O8b\n",
            "No      1259\n",
            "Loss     173\n",
            "New       68\n",
            "Name: count, dtype: int64\n",
            "Available columns:\n",
            "['D2', 'region', 'O1', 'O8b', 'P1', 'D1', 'D5', 'D67', 'D4', 'D11', 'D8', 'E3', 'H2', 'H1Ax', 'H4', 'H56', 'S', 'QOL', 'GH', 'L1', 'H3', 'Q2', 'Q1_cat']\n",
            "Treatment value counts:\n",
            "treatment\n",
            "0.0    1259\n",
            "NaN     173\n",
            "1.0      68\n",
            "Name: count, dtype: int64\n",
            "Shape after filtering treatment: (1327, 24)\n",
            "Available confounders: ['D4', 'D5', 'D67', 'D11', 'H2', 'H1Ax', 'H56', 'P1', 'region']\n",
            "Missing values before imputation:\n",
            "QOL          0\n",
            "treatment    0\n",
            "D4           0\n",
            "D5           0\n",
            "D67          0\n",
            "D11          0\n",
            "H2           0\n",
            "H1Ax         0\n",
            "H56          0\n",
            "P1           0\n",
            "region       0\n",
            "dtype: int64\n",
            "Sample data for categorical confounders:\n",
            "      D4           D5         D67    region   H2 H1Ax H56\n",
            "0    Men  High school      Others  Atlantic   No   No  No\n",
            "1    Men      College  Caucasians  Prairies   No   No  No\n",
            "2    Men   University  Caucasians  Atlantic  Yes  Yes  No\n",
            "3    Men  High school  Caucasians   Ontario   No   No  No\n",
            "4  Women   University  Caucasians   Ontario   No   No  No\n",
            "One-hot encoding categorical variables: ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
            "Confounder columns after encoding: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11', 'P1']\n",
            "Continuous variables: ['D11', 'P1']\n",
            "-utils continuous variables (non-empty): ['D11', 'P1']\n",
            "Imputing and standardizing: ['D11', 'P1']\n",
            "Missing values in categorical confounders: False\n",
            "Data types of confounder columns:\n",
            "D4_Other or did not answer      int64\n",
            "D4_Women                        int64\n",
            "D5_College                      int64\n",
            "D5_High school                  int64\n",
            "D5_University                   int64\n",
            "D67_Others                      int64\n",
            "region_British Columbia         int64\n",
            "region_Ontario                  int64\n",
            "region_Prairies                 int64\n",
            "region_Qu√©bec                   int64\n",
            "H2_Yes                          int64\n",
            "H1Ax_Yes                        int64\n",
            "H56_Yes                         int64\n",
            "D11                           float64\n",
            "P1                            float64\n",
            "dtype: object\n",
            "Missing values after imputation:\n",
            "QOL                           0\n",
            "treatment                     0\n",
            "D4_Other or did not answer    0\n",
            "D4_Women                      0\n",
            "D5_College                    0\n",
            "D5_High school                0\n",
            "D5_University                 0\n",
            "D67_Others                    0\n",
            "region_British Columbia       0\n",
            "region_Ontario                0\n",
            "region_Prairies               0\n",
            "region_Qu√©bec                 0\n",
            "H2_Yes                        0\n",
            "H1Ax_Yes                      0\n",
            "H56_Yes                       0\n",
            "D11                           0\n",
            "P1                            0\n",
            "dtype: int64\n",
            "Final shape after preprocessing: (1327, 30)\n",
            "\n",
            "Estimating ATT with IPTW...\n",
            "IPTW ATT = -0.014, SE = 0.018, 95% CI = (-0.048, 0.021)\n",
            "\n",
            "Performing propensity score matching...\n",
            "Shape of matched dataset: (136, 31)\n",
            "Matched ATE (AIPTW) = 0.001, SE = 0.016, 95% CI = (-0.031, 0.033)\n",
            "Matched ATT = -0.012, SE = 0.022, 95% CI = (-0.056, 0.032)\n",
            "\n",
            "Performing stratified analysis...\n",
            "O1 value counts:\n",
            "O1\n",
            "No     695\n",
            "Yes    632\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Stratified analysis for Dog owners:\n",
            "Skipping Dog: insufficient data (n=0)\n",
            "\n",
            "Stratified analysis for Cat owners:\n",
            "Skipping Cat: insufficient data (n=0)\n",
            "\n",
            "Trying binary O1 (pet owners vs. non-owners)...\n",
            "Pet Owners ATT = -0.003, SE = 0.018, 95% CI = (-0.037, 0.032)\n",
            "\n",
            "Computing E-value for IPTW ATT...\n",
            "E-value (point estimate) = 1.12\n",
            "E-value (lower CI bound) = 1.26\n",
            "\n",
            "Generating Austin plot for sensitivity analysis (iptw_att)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-cd0bcf1ffdd0>:174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['weight'] = weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing SMD for IPTW ATT...\n",
            "SMD for D4_Other or did not answer: 0.123\n",
            "SMD for D4_Women: 0.090\n",
            "SMD for D5_College: 0.117\n",
            "SMD for D5_High school: 0.125\n",
            "SMD for D5_University: -0.207\n",
            "SMD for D67_Others: -0.195\n",
            "SMD for region_British Columbia: -0.295\n",
            "SMD for region_Ontario: -0.074\n",
            "SMD for region_Prairies: 0.089\n",
            "SMD for region_Qu√©bec: 0.186\n",
            "SMD for H2_Yes: -0.073\n",
            "SMD for H1Ax_Yes: 0.139\n",
            "SMD for H56_Yes: 0.136\n",
            "SMD for D11: 0.198\n",
            "SMD for P1: 0.507\n",
            "Diagnostic plots saved in 'plots' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.colors as mcolors\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def preprocess_data(data, o8b_treated='New', o8b_control='No'):\n",
        "    print(\"Starting preprocessing...\")\n",
        "    df = data.copy()\n",
        "\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "    print(\"O8b value counts:\")\n",
        "    print(df['O8b'].value_counts(dropna=False))\n",
        "\n",
        "    print(\"Available columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    df['treatment'] = np.where(df['O8b'] == o8b_treated, 1,\n",
        "                             np.where(df['O8b'] == o8b_control, 0, np.nan))\n",
        "\n",
        "    print(\"Treatment value counts:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "\n",
        "    df = df[df['treatment'].notna()]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(f\"Shape after filtering treatment: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows remain after filtering O8b. Check o8b_treated and o8b_control values.\")\n",
        "\n",
        "    outcome = 'QOL'\n",
        "    if outcome not in df.columns:\n",
        "        raise ValueError(f\"Outcome column '{outcome}' not found in data.\")\n",
        "\n",
        "    confounders = ['D4', 'D5', 'D67', 'D11', 'H2', 'H1Ax', 'H56', 'P1', 'region']\n",
        "    confounders = [col for col in confounders if col in df.columns]\n",
        "    print(f\"Available confounders: {confounders}\")\n",
        "\n",
        "    print(\"Missing values before imputation:\")\n",
        "    print(df[[outcome, 'treatment'] + confounders].isna().sum())\n",
        "\n",
        "    print(\"Sample data for categorical confounders:\")\n",
        "    categorical_sample = [c for c in ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56'] if c in df.columns]\n",
        "    if categorical_sample:\n",
        "        print(df[categorical_sample].head())\n",
        "\n",
        "    categorical_vars = ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
        "    categorical_vars = [var for var in categorical_vars if var in confounders]\n",
        "    print(f\"One-hot encoding categorical variables: {categorical_vars}\")\n",
        "    if categorical_vars:\n",
        "        df = pd.get_dummies(df, columns=categorical_vars, drop_first=True, dtype=int)\n",
        "\n",
        "    confounder_cols = [col for col in df.columns if any(col.startswith(var + '_') for var in categorical_vars)]\n",
        "    confounder_cols += [var for var in confounders if var not in categorical_vars]\n",
        "    print(f\"Confounder columns after encoding: {confounder_cols}\")\n",
        "\n",
        "    continuous_vars = ['D11', 'P1']\n",
        "    continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
        "    print(f\"Continuous variables: {continuous_vars}\")\n",
        "\n",
        "    for var in continuous_vars:\n",
        "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "    valid_continuous_vars = [var for var in continuous_vars if var in df.columns and df[var].notna().any()]\n",
        "    print(f\"Valid continuous variables (non-empty): {valid_continuous_vars}\")\n",
        "\n",
        "    if valid_continuous_vars:\n",
        "        print(f\"Imputing and standardizing: {valid_continuous_vars}\")\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        df[valid_continuous_vars] = imputer.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        df[valid_continuous_vars] = scaler.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "    categorical_confounders = [col for col in confounder_cols if col not in continuous_vars]\n",
        "    if categorical_confounders:\n",
        "        missing_cat = df[categorical_confounders].isna().any().any()\n",
        "        print(f\"Missing values in categorical confounders: {missing_cat}\")\n",
        "        if missing_cat:\n",
        "            print(f\"Imputing categorical confounders: {categorical_confounders}\")\n",
        "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "            df[categorical_confounders] = imputer_cat.fit_transform(df[categorical_confounders])\n",
        "\n",
        "    if df[outcome].isna().any():\n",
        "        print(\"Imputing missing QOL values...\")\n",
        "        imputer_outcome = SimpleImputer(strategy='mean')\n",
        "        df[outcome] = imputer_outcome.fit_transform(df[[outcome]])\n",
        "\n",
        "    print(\"Data types of confounder columns:\")\n",
        "    print(df[confounder_cols].dtypes)\n",
        "\n",
        "    non_numeric_cols = df[confounder_cols].select_dtypes(exclude=['int64', 'float64', 'uint8', 'int32']).columns\n",
        "    if non_numeric_cols.any():\n",
        "        raise ValueError(f\"Non-numeric confounders detected: {non_numeric_cols.tolist()}\")\n",
        "\n",
        "    print(\"Missing values after imputation:\")\n",
        "    final_cols = [outcome, 'treatment'] + (confounder_cols if confounder_cols else [])\n",
        "    print(df[final_cols].isna().sum())\n",
        "\n",
        "    print(f\"Final shape after preprocessing: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No data remains after preprocessing.\")\n",
        "\n",
        "    return df, outcome, confounder_cols\n",
        "\n",
        "# Step 2: Propensity Score Matching (Fixed)\n",
        "def propensity_score_matching(df, confounders):\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(df[confounders], df['treatment'])\n",
        "    ps = ps_model.predict_proba(df[confounders])[:, 1]\n",
        "\n",
        "    treated_idx = df[df['treatment'] == 1].index\n",
        "    control_idx = df[df['treatment'] == 0].index\n",
        "\n",
        "    treated_ps = ps[treated_idx]\n",
        "    control_ps = ps[control_idx]\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=1)\n",
        "    nn.fit(control_ps.reshape(-1, 1))\n",
        "    distances, indices = nn.kneighbors(treated_ps.reshape(-1, 1))\n",
        "\n",
        "    matched_control_idx = control_idx[indices.flatten()]\n",
        "    matched_df = df.loc[list(treated_idx) + list(matched_control_idx)]\n",
        "\n",
        "    print(f\"Shape of matched dataset: {matched_df.shape}\")\n",
        "    return matched_df, ps\n",
        "\n",
        "# Step 3: IPTW for ATT\n",
        "def estimate_iptw_att(df, outcome, confounders):\n",
        "    T = df['treatment'].values\n",
        "    Y = df[outcome].values\n",
        "    X = df[confounders].values if confounders else np.ones((len(df), 1))\n",
        "\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(X, T)\n",
        "    ps = ps_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    weights = np.where(T == 1, 1, ps / (1 - ps))\n",
        "    weights = np.clip(weights, 0.1, 10)\n",
        "\n",
        "    treated_idx = T == 1\n",
        "    control_idx = T == 0\n",
        "\n",
        "    mu1 = np.mean(Y[treated_idx])\n",
        "    mu0 = np.average(Y[control_idx], weights=weights[control_idx])\n",
        "    att = mu1 - mu0\n",
        "\n",
        "    n = len(Y)\n",
        "    boot_atts = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        T_boot = T[idx]\n",
        "        Y_boot = Y[idx]\n",
        "        weights_boot = weights[idx]\n",
        "        treated_boot = T_boot == 1\n",
        "        control_boot = T_boot == 0\n",
        "        mu1_boot = np.mean(Y_boot[treated_boot])\n",
        "        mu0_boot = np.average(Y_boot[control_boot], weights=weights_boot[control_boot])\n",
        "        boot_atts.append(mu1_boot - mu0_boot)\n",
        "\n",
        "    se = np.std(boot_atts)\n",
        "    ci = (att - 1.96 * se, att + 1.96 * se)\n",
        "\n",
        "    df['weight'] = weights\n",
        "    return att, se, ci, ps\n",
        "\n",
        "# Step 4: AIPTW (for Matched Data)\n",
        "def estimate_aiptw(df, outcome, confounders):\n",
        "    T = df['treatment'].values\n",
        "    Y = df[outcome].values\n",
        "    X = df[confounders].values if confounders else np.ones((len(df), 1))\n",
        "\n",
        "    ps_model = LogisticRegression(max_iter=1000).fit(X, T)\n",
        "    ps = ps_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    treated_idx = T == 1\n",
        "    control_idx = T == 0\n",
        "\n",
        "    outcome_model_treated = LinearRegression().fit(X[treated_idx], Y[treated_idx])\n",
        "    outcome_model_control = LinearRegression().fit(X[control_idx], Y[control_idx])\n",
        "\n",
        "    mu1_hat = outcome_model_treated.predict(X)\n",
        "    mu0_hat = outcome_model_control.predict(X)\n",
        "\n",
        "    term1 = (T * Y / ps) + (1 - T / ps) * mu1_hat\n",
        "    term0 = ((1 - T) * Y / (1 - ps)) + (1 - (1 - T) / (1 - ps)) * mu0_hat\n",
        "\n",
        "    mu1 = np.mean(term1)\n",
        "    mu0 = np.mean(term0)\n",
        "    ate = mu1 - mu0\n",
        "\n",
        "    n = len(Y)\n",
        "    boot_ates = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        T_boot = T[idx]\n",
        "        Y_boot = Y[idx]\n",
        "        X_boot = X[idx]\n",
        "        ps_boot = ps_model.predict_proba(X_boot)[:, 1]\n",
        "        mu1_boot = outcome_model_treated.predict(X_boot)\n",
        "        mu0_boot = outcome_model_control.predict(X_boot)\n",
        "        term1_boot = (T_boot * Y_boot / ps_boot) + (1 - T_boot / ps_boot) * mu1_boot\n",
        "        term0_boot = ((1 - T_boot) * Y_boot / (1 - ps_boot)) + (1 - (1 - T_boot) / (1 - ps_boot)) * mu0_boot\n",
        "        boot_ates.append(np.mean(term1_boot) - np.mean(term0_boot))\n",
        "\n",
        "    se = np.std(boot_ates)\n",
        "    ci = (ate - 1.96 * se, ate + 1.96 * se)\n",
        "\n",
        "    return ate, se, ci, ps\n",
        "\n",
        "# Step 5: Matching for ATT\n",
        "def matching_att(df, outcome, ps):\n",
        "    treated_idx = df[df['treatment'] == 1].index\n",
        "    control_idx = df[df['treatment'] == 0].index\n",
        "\n",
        "    treated_ps = ps[treated_idx]\n",
        "    control_ps = ps[control_idx]\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=1)\n",
        "    nn.fit(control_ps.reshape(-1, 1))\n",
        "    distances, indices = nn.kneighbors(treated_ps.reshape(-1, 1))\n",
        "\n",
        "    matched_control_idx = control_idx[indices.flatten()]\n",
        "    matched_df = df.loc[list(treated_idx) + list(matched_control_idx)]\n",
        "\n",
        "    att = matched_df[matched_df['treatment'] == 1][outcome].mean() - matched_df[matched_df['treatment'] == 0][outcome].mean()\n",
        "\n",
        "    boot_atts = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(len(matched_df), len(matched_df), replace=True)\n",
        "        boot_df = matched_df.iloc[idx]\n",
        "        att_boot = boot_df[boot_df['treatment'] == 1][outcome].mean() - boot_df[boot_df['treatment'] == 0][outcome].mean()\n",
        "        boot_atts.append(att_boot)\n",
        "\n",
        "    se = np.std(boot_atts)\n",
        "    ci = (att - 1.96 * se, att + 1.96 * se)\n",
        "\n",
        "    return att, se, ci\n",
        "\n",
        "# Step 6: Stratified Analysis by Pet Type\n",
        "def stratified_analysis(df, outcome, confounders):\n",
        "    print(\"O1 value counts:\")\n",
        "    print(df['O1'].value_counts(dropna=False))\n",
        "\n",
        "    pet_types = ['Dog', 'Cat']\n",
        "    results = {}\n",
        "\n",
        "    for pet_type in pet_types:\n",
        "        print(f\"\\nStratified analysis for {pet_type} owners:\")\n",
        "        sub_df = df[df['O1'].str.contains(pet_type, case=False, na=False)]\n",
        "        if len(sub_df) < 10 or sub_df['treatment'].nunique() < 2:\n",
        "            print(f\"Skipping {pet_type}: insufficient data (n={len(sub_df)})\")\n",
        "            continue\n",
        "\n",
        "        att, se, ci, ps = estimate_iptw_att(sub_df, outcome, confounders)\n",
        "        results[pet_type] = {'ATT': att, 'SE': se, 'CI': ci}\n",
        "        print(f\"{pet_type} ATT = {att:.3f}, SE = {se:.3f}, 95% CI = ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
        "\n",
        "    if not results:\n",
        "        print(\"\\nTrying binary O1 (pet owners vs. non-owners)...\")\n",
        "        pet_owners = df[df['O1'].str.contains('Yes', case=False, na=False)]\n",
        "        if len(pet_owners) >= 10 and pet_owners['treatment'].nunique() == 2:\n",
        "            att, se, ci, ps = estimate_iptw_att(pet_owners, outcome, confounders)\n",
        "            results['Pet Owners'] = {'ATT': att, 'SE': se, 'CI': ci}\n",
        "            print(f\"Pet Owners ATT = {att:.3f}, SE = {se:.3f}, 95% CI = ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Step 7: Sensitivity Analysis (E-value and Austin Plot with Bias Curve)\n",
        "def compute_e_value(ate, se):\n",
        "    point_est = abs(ate)\n",
        "    rr_point = np.exp(0.91 * point_est)\n",
        "    e_value_point = rr_point + np.sqrt(rr_point * (rr_point - 1))\n",
        "\n",
        "    lower_bound = abs(ate - 1.96 * se)\n",
        "    if lower_bound > 0:\n",
        "        rr_lower = np.exp(0.91 * lower_bound)\n",
        "        e_value_lower = rr_lower + np.sqrt(rr_lower * (rr_lower - 1))\n",
        "    else:\n",
        "        e_value_lower = 1.0\n",
        "\n",
        "    return e_value_point, e_value_lower\n",
        "\n",
        "# Calculate Partial R^2 for Observed Confounders\n",
        "def calculate_partial_r2(df, confounder_cols, target):\n",
        "    r2_values = []\n",
        "    for col in confounder_cols:\n",
        "        X = df[[col]].dropna()\n",
        "        y = df.loc[X.index, target]\n",
        "        if len(X) == 0 or y.isna().all():\n",
        "            continue\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        r2_values.append((col, r2))\n",
        "    return r2_values\n",
        "\n",
        "# Austin Plot with Bias Curve Styled Like the Provided Code\n",
        "def austin_plot_sensitivity(df, outcome, confounder_cols, att_orig, se_orig, output_dir='plots', prefix=''):\n",
        "    print(f\"\\nGenerating Austin plot with bias curve for sensitivity analysis ({prefix})...\")\n",
        "\n",
        "    T = df['treatment'].values\n",
        "    Y = df[outcome].values\n",
        "\n",
        "    # Calculate partial R^2 for observed confounders\n",
        "    r2_t = calculate_partial_r2(df, confounder_cols, 'treatment')\n",
        "    r2_y = calculate_partial_r2(df, confounder_cols, outcome)\n",
        "\n",
        "    # Create a dictionary mapping confounder to (R^2_UT, R^2_UY)\n",
        "    confounder_r2 = {}\n",
        "    for (conf_t, r2_ut), (conf_y, r2_uy) in zip(r2_t, r2_y):\n",
        "        if conf_t == conf_y:\n",
        "            confounder_r2[conf_t] = (r2_ut, r2_uy)\n",
        "\n",
        "    print(\"\\nPartial R^2 of observed confounders:\")\n",
        "    for conf, (r2_ut, r2_uy) in confounder_r2.items():\n",
        "        print(f\"{conf}: R^2_UT = {r2_ut:.3f}, R^2_UY = {r2_uy:.3f}\")\n",
        "\n",
        "    # Generate distinct colors for each confounder\n",
        "    colors = list(mcolors.TABLEAU_COLORS.values())[:len(confounder_r2)]\n",
        "    if len(colors) < len(confounder_r2):\n",
        "        colors.extend(list(mcolors.CSS4_COLORS.values())[:len(confounder_r2) - len(colors)])\n",
        "\n",
        "    # Simulate unmeasured confounder U\n",
        "    df['U'] = np.random.normal(0, 1, len(df))\n",
        "\n",
        "    # Vary R^2 values for unmeasured confounding\n",
        "    r2_values = np.linspace(0.0, 0.5, 21)  # 0.0 to 0.5 in 21 steps for contour plot\n",
        "    r2_diagonal = np.linspace(0.0, 0.5, 50)  # Finer grid for the bias curve\n",
        "    adjusted_atts = np.zeros((len(r2_values), len(r2_values)))\n",
        "\n",
        "    sd_y = np.std(df[outcome])\n",
        "    sd_t = np.std(df['treatment'])\n",
        "\n",
        "    # Compute adjusted ATT for the contour plot\n",
        "    for i, r2_ut in enumerate(r2_values):\n",
        "        for j, r2_uy in enumerate(r2_values):\n",
        "            df['T_adj'] = np.sqrt(1 - r2_ut) * df['treatment'] + np.sqrt(r2_ut) * df['U'] * sd_t\n",
        "            df['Y_adj'] = df[outcome] + np.sqrt(r2_uy) * df['U'] * sd_y\n",
        "            weights = np.where(df['T_adj'] > 0.5, 1, 0.5 / (1 - 0.5))\n",
        "            treated_idx = df['T_adj'] > 0.5\n",
        "            control_idx = df['T_adj'] <= 0.5\n",
        "            mu1 = np.mean(df['Y_adj'][treated_idx])\n",
        "            mu0 = np.average(df['Y_adj'][control_idx], weights=weights[control_idx])\n",
        "            adjusted_atts[i, j] = mu1 - mu0\n",
        "\n",
        "    # Compute ATT along the diagonal for the bias curve (R^2_UT = R^2_UY)\n",
        "    diagonal_atts = []\n",
        "    for r2 in r2_diagonal:\n",
        "        df['T_adj'] = np.sqrt(1 - r2) * df['treatment'] + np.sqrt(r2) * df['U'] * sd_t\n",
        "        df['Y_adj'] = df[outcome] + np.sqrt(r2) * df['U'] * sd_y\n",
        "        weights = np.where(df['T_adj'] > 0.5, 1, 0.5 / (1 - 0.5))\n",
        "        treated_idx = df['T_adj'] > 0.5\n",
        "        control_idx = df['T_adj'] <= 0.5\n",
        "        mu1 = np.mean(df['Y_adj'][treated_idx])\n",
        "        mu0 = np.average(df['Y_adj'][control_idx], weights=weights[control_idx])\n",
        "        diagonal_atts.append(mu1 - mu0)\n",
        "\n",
        "    # Compute bias grid (Adjusted ATT - Original ATT)\n",
        "    bias_grid = adjusted_atts - att_orig\n",
        "\n",
        "    # Generate Austin Plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Contour plot of adjusted ATT values\n",
        "    contour = plt.contourf(r2_values, r2_values, adjusted_atts, levels=20, cmap='viridis')\n",
        "    plt.colorbar(contour, label=f'Adjusted ATT', pad=0.1, fraction=0.046)\n",
        "\n",
        "    # Contour for ATT = 0\n",
        "    cs = plt.contour(r2_values, r2_values, adjusted_atts, levels=[0], colors='red', linestyles='dashed', linewidths=2)\n",
        "    plt.clabel(cs, inline=True, fontsize=10, fmt='ATT = 0', colors='red')\n",
        "\n",
        "    # Contours for 95% CI bounds, highlight lower CI as bias curve\n",
        "    ci_lower = att_orig - 1.96 * se_orig\n",
        "    ci_upper = att_orig + 1.96 * se_orig\n",
        "    cs_ci = plt.contour(r2_values, r2_values, adjusted_atts, levels=[ci_lower, ci_upper], colors=['cyan', 'white'], linestyles=['solid', 'dotted'], linewidths=[2, 2])\n",
        "    plt.clabel(cs_ci, inline=True, fontsize=10, fmt={ci_lower: f'Bias = Lower CI ({ci_lower:.3f})', ci_upper: f'CI Upper ({ci_upper:.3f})'}, colors=['cyan', 'white'])\n",
        "\n",
        "    # Contour for bias = -1.96 * SE (should match the lower CI contour)\n",
        "    bias_lower_ci = -1.96 * se_orig\n",
        "    cs_bias = plt.contour(r2_values, r2_values, bias_grid, levels=[bias_lower_ci], colors='cyan', linestyles='solid', linewidths=2)\n",
        "    plt.clabel(cs_bias, inline=True, fontsize=10, fmt=f'Bias = {bias_lower_ci:.3f}', colors='cyan')\n",
        "\n",
        "    # Plot bias curve along the diagonal (R^2_UT = R^2_UY)\n",
        "    plt.plot(r2_diagonal, r2_diagonal, linestyle='--', color='black', linewidth=2, label='R¬≤_UT = R¬≤_UY Path')\n",
        "\n",
        "    # Plot observed confounders as scatter points\n",
        "    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', 'h', '*']\n",
        "    for idx, (conf, (r2_ut, r2_uy)) in enumerate(confounder_r2.items()):\n",
        "        plt.scatter(r2_ut, r2_uy, label=conf, color=colors[idx % len(colors)],\n",
        "                    marker=markers[idx % len(markers)], s=150, alpha=0.8, edgecolors='black')\n",
        "\n",
        "    # Add annotations\n",
        "    plt.text(0.02, 0.48, f'Original ATT: {att_orig:.3f}', color='white', fontsize=12, fontweight='bold',\n",
        "             bbox=dict(facecolor='black', alpha=0.5))\n",
        "    plt.text(0.02, 0.44, f'CI: ({ci_lower:.3f}, {ci_upper:.3f})', color='white', fontsize=12, fontweight='bold',\n",
        "             bbox=dict(facecolor='black', alpha=0.5))\n",
        "\n",
        "    # Set axis labels and title\n",
        "    plt.xlabel('Influence on Treatment (R¬≤_UT)', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Influence on Outcome (R¬≤_UY)', fontsize=14, fontweight='bold')\n",
        "    plt.title(f'Austin Plot for Sensitivity to Unmeasured Confounding\\n(Points Represent Observed Confounders)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    # Customize legend\n",
        "    num_confounders = len(confounder_r2) + 1  # +1 for the diagonal path\n",
        "    num_cols = min(3, max(1, num_confounders // 4 + 1))\n",
        "    plt.legend(title=\"Observed Confounders\", fontsize=10, title_fontsize=12,\n",
        "               loc='upper center', bbox_to_anchor=(0.5, -0.15),\n",
        "               ncol=num_cols, frameon=True, edgecolor='black', markerscale=1.5)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.25)\n",
        "    plt.savefig(f'{output_dir}/austen_plot_with_bias_{prefix}.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Step 8: Diagnostics with SMD\n",
        "def compute_smd(df, confounders, weights):\n",
        "    treated = df[df['treatment'] == 1]\n",
        "    control = df[df['treatment'] == 0]\n",
        "    weights_t = weights[df['treatment'] == 1]\n",
        "    weights_c = weights[df['treatment'] == 0]\n",
        "    smd_results = {}\n",
        "\n",
        "    for col in confounders:\n",
        "        mean_t = np.average(treated[col], weights=weights_t)\n",
        "        mean_c = np.average(control[col], weights=weights_c)\n",
        "        var_t = np.average((treated[col] - mean_t)**2, weights=weights_t)\n",
        "        var_c = np.average((control[col] - mean_c)**2, weights=weights_c)\n",
        "        smd = (mean_t - mean_c) / np.sqrt((var_t + var_c) / 2)\n",
        "        smd_results[col] = smd\n",
        "\n",
        "    return smd_results\n",
        "\n",
        "def plot_diagnostics(ps, df, output_dir='plots', prefix=''):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(ps[df['treatment'] == 1], label='Treated', color='blue', alpha=0.5)\n",
        "    sns.histplot(ps[df['treatment'] == 0], label='Control', color='orange', alpha=0.5)\n",
        "    plt.title(f'Propensity Score Distribution ({prefix})')\n",
        "    plt.xlabel('Propensity Score')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{output_dir}/{prefix}_ps_overlap.png')\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(df['weight'], color='green', alpha=0.5)\n",
        "    plt.title(f'IPTW Weight Distribution ({prefix})')\n",
        "    plt.xlabel('Weight')\n",
        "    plt.savefig(f'{output_dir}/{prefix}_weight_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "# Main Function\n",
        "def main(data, o8b_treated='New', o8b_control='No'):\n",
        "    df, outcome, confounders = preprocess_data(data, o8b_treated, o8b_control)\n",
        "\n",
        "    print(\"\\nEstimating ATT with IPTW...\")\n",
        "    att_iptw, se_iptw, ci_iptw, ps = estimate_iptw_att(df, outcome, confounders)\n",
        "    print(f\"IPTW ATT = {att_iptw:.3f}, SE = {se_iptw:.3f}, 95% CI = ({ci_iptw[0]:.3f}, {ci_iptw[1]:.3f})\")\n",
        "\n",
        "    print(\"\\nPerforming propensity score matching...\")\n",
        "    matched_df, ps_matched = propensity_score_matching(df, confounders)\n",
        "    ate_matched, se_matched, ci_matched, ps_matched_new = estimate_aiptw(matched_df, outcome, confounders)\n",
        "    print(f\"Matched ATE (AIPTW) = {ate_matched:.3f}, SE = {se_matched:.3f}, 95% CI = ({ci_matched[0]:.3f}, {ci_matched[1]:.3f})\")\n",
        "\n",
        "    att_matched, se_matched_att, ci_matched_att = matching_att(df, outcome, ps)\n",
        "    print(f\"Matched ATT = {att_matched:.3f}, SE = {se_matched_att:.3f}, 95% CI = ({ci_matched_att[0]:.3f}, {ci_matched_att[1]:.3f})\")\n",
        "\n",
        "    print(\"\\nPerforming stratified analysis...\")\n",
        "    stratified_results = stratified_analysis(df, outcome, confounders)\n",
        "\n",
        "    print(\"\\nComputing E-value for IPTW ATT...\")\n",
        "    e_value_point, e_value_lower = compute_e_value(att_iptw, se_iptw)\n",
        "    print(f\"E-value (point estimate) = {e_value_point:.2f}\")\n",
        "    print(f\"E-value (lower CI bound) = {e_value_lower:.2f}\")\n",
        "\n",
        "    # Generate Austin Plot with Bias Curve\n",
        "    austin_plot_sensitivity(df, outcome, confounders, att_iptw, se_iptw, prefix='iptw_att')\n",
        "\n",
        "    print(\"\\nComputing SMD for IPTW ATT...\")\n",
        "    smd_results = compute_smd(df, confounders, df['weight'])\n",
        "    for col, smd in smd_results.items():\n",
        "        print(f\"SMD for {col}: {smd:.3f}\")\n",
        "\n",
        "    plot_diagnostics(ps, df, prefix='full')\n",
        "    plot_diagnostics(ps_matched_new, matched_df, prefix='matched')\n",
        "    print(\"Diagnostic plots saved in 'plots' directory.\")\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    main(data, o8b_treated='New', o8b_control='No')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Teqth2ksElU",
        "outputId": "4b53b160-4a83-4493-ec94-faaf27ffa397"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing...\n",
            "Initial shape: (1500, 23)\n",
            "O8b value counts:\n",
            "O8b\n",
            "No      1259\n",
            "Loss     173\n",
            "New       68\n",
            "Name: count, dtype: int64\n",
            "Available columns:\n",
            "['D2', 'region', 'O1', 'O8b', 'P1', 'D1', 'D5', 'D67', 'D4', 'D11', 'D8', 'E3', 'H2', 'H1Ax', 'H4', 'H56', 'S', 'QOL', 'GH', 'L1', 'H3', 'Q2', 'Q1_cat']\n",
            "Treatment value counts:\n",
            "treatment\n",
            "0.0    1259\n",
            "NaN     173\n",
            "1.0      68\n",
            "Name: count, dtype: int64\n",
            "Shape after filtering treatment: (1327, 24)\n",
            "Available confounders: ['D4', 'D5', 'D67', 'D11', 'H2', 'H1Ax', 'H56', 'P1', 'region']\n",
            "Missing values before imputation:\n",
            "QOL          0\n",
            "treatment    0\n",
            "D4           0\n",
            "D5           0\n",
            "D67          0\n",
            "D11          0\n",
            "H2           0\n",
            "H1Ax         0\n",
            "H56          0\n",
            "P1           0\n",
            "region       0\n",
            "dtype: int64\n",
            "Sample data for categorical confounders:\n",
            "      D4           D5         D67    region   H2 H1Ax H56\n",
            "0    Men  High school      Others  Atlantic   No   No  No\n",
            "1    Men      College  Caucasians  Prairies   No   No  No\n",
            "2    Men   University  Caucasians  Atlantic  Yes  Yes  No\n",
            "3    Men  High school  Caucasians   Ontario   No   No  No\n",
            "4  Women   University  Caucasians   Ontario   No   No  No\n",
            "One-hot encoding categorical variables: ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
            "Confounder columns after encoding: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11', 'P1']\n",
            "Continuous variables: ['D11', 'P1']\n",
            "Valid continuous variables (non-empty): ['D11', 'P1']\n",
            "Imputing and standardizing: ['D11', 'P1']\n",
            "Missing values in categorical confounders: False\n",
            "Data types of confounder columns:\n",
            "D4_Other or did not answer      int64\n",
            "D4_Women                        int64\n",
            "D5_College                      int64\n",
            "D5_High school                  int64\n",
            "D5_University                   int64\n",
            "D67_Others                      int64\n",
            "region_British Columbia         int64\n",
            "region_Ontario                  int64\n",
            "region_Prairies                 int64\n",
            "region_Qu√©bec                   int64\n",
            "H2_Yes                          int64\n",
            "H1Ax_Yes                        int64\n",
            "H56_Yes                         int64\n",
            "D11                           float64\n",
            "P1                            float64\n",
            "dtype: object\n",
            "Missing values after imputation:\n",
            "QOL                           0\n",
            "treatment                     0\n",
            "D4_Other or did not answer    0\n",
            "D4_Women                      0\n",
            "D5_College                    0\n",
            "D5_High school                0\n",
            "D5_University                 0\n",
            "D67_Others                    0\n",
            "region_British Columbia       0\n",
            "region_Ontario                0\n",
            "region_Prairies               0\n",
            "region_Qu√©bec                 0\n",
            "H2_Yes                        0\n",
            "H1Ax_Yes                      0\n",
            "H56_Yes                       0\n",
            "D11                           0\n",
            "P1                            0\n",
            "dtype: int64\n",
            "Final shape after preprocessing: (1327, 30)\n",
            "\n",
            "Estimating ATT with IPTW...\n",
            "IPTW ATT = -0.014, SE = 0.018, 95% CI = (-0.048, 0.021)\n",
            "\n",
            "Performing propensity score matching...\n",
            "Shape of matched dataset: (136, 31)\n",
            "Matched ATE (AIPTW) = 0.001, SE = 0.016, 95% CI = (-0.031, 0.033)\n",
            "Matched ATT = -0.012, SE = 0.022, 95% CI = (-0.056, 0.032)\n",
            "\n",
            "Performing stratified analysis...\n",
            "O1 value counts:\n",
            "O1\n",
            "No     695\n",
            "Yes    632\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Stratified analysis for Dog owners:\n",
            "Skipping Dog: insufficient data (n=0)\n",
            "\n",
            "Stratified analysis for Cat owners:\n",
            "Skipping Cat: insufficient data (n=0)\n",
            "\n",
            "Trying binary O1 (pet owners vs. non-owners)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-4a44afc81b9d>:173: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['weight'] = weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pet Owners ATT = -0.003, SE = 0.018, 95% CI = (-0.037, 0.032)\n",
            "\n",
            "Computing E-value for IPTW ATT...\n",
            "E-value (point estimate) = 1.12\n",
            "E-value (lower CI bound) = 1.26\n",
            "\n",
            "Generating Austin plot with bias curve for sensitivity analysis (iptw_att)...\n",
            "\n",
            "Partial R^2 of observed confounders:\n",
            "D4_Other or did not answer: R^2_UT = 0.002, R^2_UY = 0.000\n",
            "D4_Women: R^2_UT = 0.001, R^2_UY = 0.004\n",
            "D5_College: R^2_UT = 0.001, R^2_UY = 0.003\n",
            "D5_High school: R^2_UT = 0.001, R^2_UY = 0.003\n",
            "D5_University: R^2_UT = 0.003, R^2_UY = 0.016\n",
            "D67_Others: R^2_UT = 0.002, R^2_UY = 0.002\n",
            "region_British Columbia: R^2_UT = 0.003, R^2_UY = 0.000\n",
            "region_Ontario: R^2_UT = 0.000, R^2_UY = 0.001\n",
            "region_Prairies: R^2_UT = 0.001, R^2_UY = 0.001\n",
            "region_Qu√©bec: R^2_UT = 0.002, R^2_UY = 0.006\n",
            "H2_Yes: R^2_UT = 0.000, R^2_UY = 0.214\n",
            "H1Ax_Yes: R^2_UT = 0.001, R^2_UY = 0.186\n",
            "H56_Yes: R^2_UT = 0.002, R^2_UY = 0.000\n",
            "D11: R^2_UT = 0.003, R^2_UY = 0.001\n",
            "P1: R^2_UT = 0.011, R^2_UY = 0.021\n",
            "\n",
            "Computing SMD for IPTW ATT...\n",
            "SMD for D4_Other or did not answer: 0.123\n",
            "SMD for D4_Women: 0.090\n",
            "SMD for D5_College: 0.117\n",
            "SMD for D5_High school: 0.125\n",
            "SMD for D5_University: -0.207\n",
            "SMD for D67_Others: -0.195\n",
            "SMD for region_British Columbia: -0.295\n",
            "SMD for region_Ontario: -0.074\n",
            "SMD for region_Prairies: 0.089\n",
            "SMD for region_Qu√©bec: 0.186\n",
            "SMD for H2_Yes: -0.073\n",
            "SMD for H1Ax_Yes: 0.139\n",
            "SMD for H56_Yes: 0.136\n",
            "SMD for D11: 0.198\n",
            "SMD for P1: 0.507\n",
            "Diagnostic plots saved in 'plots' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimate using 2SLS"
      ],
      "metadata": {
        "id": "7V6nEInUrPJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def preprocess_data(data):\n",
        "    print(\"Starting preprocessing...\")\n",
        "    df = data.copy()\n",
        "\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "    print(\"O1 value counts:\")\n",
        "    print(df['O1'].value_counts(dropna=False))\n",
        "\n",
        "    print(\"Available columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Define treatment (O1)\n",
        "    df['treatment'] = np.where(df['O1'] == 'Yes', 1,\n",
        "                             np.where(df['O1'] == 'No', 0, np.nan))\n",
        "\n",
        "    # Define instrument (P1) - keep as continuous\n",
        "    df['instrument'] = df['P1']\n",
        "\n",
        "    print(\"Treatment value counts:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "\n",
        "    print(\"Instrument (P1) summary:\")\n",
        "    print(df['instrument'].describe())\n",
        "\n",
        "    # Filter to keep only valid treatment and instrument\n",
        "    df = df[df['treatment'].notna() & df['instrument'].notna()]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(f\"Shape after filtering: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows remain after filtering.\")\n",
        "\n",
        "    outcome = 'QOL'\n",
        "    if outcome not in df.columns:\n",
        "        raise ValueError(f\"Outcome column '{outcome}' not found in data.\")\n",
        "\n",
        "    confounders = ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
        "    confounders = [col for col in confounders if col in df.columns]\n",
        "    print(f\"Initial confounders: {confounders}\")\n",
        "\n",
        "    # Exclude confounders with all NaN values initially\n",
        "    valid_confounders = []\n",
        "    for col in confounders:\n",
        "        if df[col].notna().any():\n",
        "            valid_confounders.append(col)\n",
        "        else:\n",
        "            print(f\"Excluding {col}: all values are NaN\")\n",
        "    confounders = valid_confounders\n",
        "    print(f\"Valid confounders: {confounders}\")\n",
        "\n",
        "    print(\"Missing values before imputation:\")\n",
        "    print(df[[outcome, 'treatment', 'instrument'] + confounders].isna().sum())\n",
        "\n",
        "    print(\"Sample data for categorical confounders:\")\n",
        "    categorical_sample = [c for c in ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56'] if c in df.columns]\n",
        "    if categorical_sample:\n",
        "        print(df[categorical_sample].head())\n",
        "\n",
        "    categorical_vars = ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
        "    categorical_vars = [var for var in categorical_vars if var in confounders]\n",
        "    print(f\"One-hot encoding categorical variables: {categorical_vars}\")\n",
        "    if categorical_vars:\n",
        "        df = pd.get_dummies(df, columns=categorical_vars, drop_first=True, dtype=int)\n",
        "\n",
        "    confounder_cols = [col for col in df.columns if any(col.startswith(var + '_') for var in categorical_vars)]\n",
        "    confounder_cols += [var for var in confounders if var not in categorical_vars]\n",
        "    print(f\"Confounder columns after encoding: {confounder_cols}\")\n",
        "\n",
        "    continuous_vars = ['D1', 'D8', 'D11']\n",
        "    continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
        "    print(f\"Continuous variables: {continuous_vars}\")\n",
        "\n",
        "    for var in continuous_vars:\n",
        "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "    valid_continuous_vars = [var for var in continuous_vars if df[var].notna().any()]\n",
        "    print(f\"Valid continuous variables (non-empty): {valid_continuous_vars}\")\n",
        "\n",
        "    if valid_continuous_vars:\n",
        "        print(f\"Imputing and standardizing: {valid_continuous_vars}\")\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        df[valid_continuous_vars] = imputer.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        df[valid_continuous_vars] = scaler.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "    categorical_confounders = [col for col in confounder_cols if col not in continuous_vars]\n",
        "    if categorical_confounders:\n",
        "        missing_cat = df[categorical_confounders].isna().any().any()\n",
        "        print(f\"Missing values in categorical confounders: {missing_cat}\")\n",
        "        if missing_cat:\n",
        "            print(f\"Imputing categorical confounders: {categorical_confounders}\")\n",
        "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "            df[categorical_confounders] = imputer_cat.fit_transform(df[categorical_confounders])\n",
        "\n",
        "    if df[outcome].isna().any():\n",
        "        print(\"Imputing missing QOL values...\")\n",
        "        imputer_outcome = SimpleImputer(strategy='mean')\n",
        "        df[outcome] = imputer_outcome.fit_transform(df[[outcome]])\n",
        "\n",
        "    # Drop columns that are entirely NaN after processing\n",
        "    df = df.dropna(axis=1, how='all')\n",
        "    print(f\"Shape after dropping entirely NaN columns: {df.shape}\")\n",
        "\n",
        "    # Update confounder_cols to only include existing columns\n",
        "    confounder_cols = [col for col in confounder_cols if col in df.columns]\n",
        "    print(f\"Updated confounder_cols: {confounder_cols}\")\n",
        "\n",
        "    print(\"Data types of confounder columns:\")\n",
        "    print(df[confounder_cols].dtypes)\n",
        "\n",
        "    non_numeric_cols = df[confounder_cols].select_dtypes(exclude=['int64', 'float64', 'uint8', 'int32']).columns\n",
        "    if non_numeric_cols.any():\n",
        "        raise ValueError(f\"Non-numeric confounders detected: {non_numeric_cols.tolist()}\")\n",
        "\n",
        "    print(\"Missing values after imputation:\")\n",
        "    final_cols = [outcome, 'treatment', 'instrument'] + confounder_cols\n",
        "    print(df[final_cols].isna().sum())\n",
        "\n",
        "    print(f\"Final shape after preprocessing: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No data remains after preprocessing.\")\n",
        "\n",
        "    return df, outcome, confounder_cols\n",
        "\n",
        "# Step 2: Instrumental Variables (IV) for ATT\n",
        "def iv_analysis_att(df, outcome, confounders):\n",
        "    print(\"\\nPerforming IV analysis for ATT...\")\n",
        "    # Check for sufficient variation in treatment and instrument\n",
        "    print(\"Treatment variation check:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "    print(\"Instrument variation check:\")\n",
        "    print(df['instrument'].describe())\n",
        "\n",
        "    if df['treatment'].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in treatment variable.\")\n",
        "    if df['instrument'].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in instrument variable.\")\n",
        "\n",
        "    # Filter confounders to only those with non-NaN values in df\n",
        "    valid_confounders = [c for c in confounders if c in df.columns and df[c].notna().any()]\n",
        "    print(f\"Valid confounders for IV analysis: {valid_confounders}\")\n",
        "\n",
        "    # First stage: Regress treatment (O1) on instrument (P1) and confounders\n",
        "    confounder_terms = [f'Q(\"{c}\")' if ' ' in c else c for c in valid_confounders if not c.startswith('region_')]\n",
        "    formula_first = 'treatment ~ instrument + ' + ' + '.join(confounder_terms)\n",
        "    print(f\"First-stage formula: {formula_first}\")\n",
        "\n",
        "    # Validate formula terms\n",
        "    for term in confounder_terms:\n",
        "        col = term.replace('Q(\"', '').replace('\")', '')\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Column {col} not in DataFrame\")\n",
        "\n",
        "    first_stage = smf.ols(formula_first, data=df).fit()\n",
        "    df['treatment_pred'] = first_stage.predict()\n",
        "\n",
        "    # Check instrument strength (F-statistic on instrument)\n",
        "    f_stat = first_stage.fvalue\n",
        "    print(f\"First-stage F-statistic: {f_stat:.2f}\")\n",
        "    if f_stat < 10:\n",
        "        print(\"Warning: Weak instrument (F < 10). IV results may be unreliable.\")\n",
        "\n",
        "    # Second stage: Regress outcome on predicted treatment and confounders\n",
        "    formula_second = f'{outcome} ~ treatment_pred + ' + ' + '.join(confounder_terms)\n",
        "    print(f\"Second-stage formula: {formula_second}\")\n",
        "    second_stage = smf.ols(formula_second, data=df).fit()\n",
        "\n",
        "    att_iv = second_stage.params['treatment_pred']\n",
        "    se_iv = second_stage.bse['treatment_pred']\n",
        "    ci_iv = (att_iv - 1.96 * se_iv, att_iv + 1.96 * se_iv)\n",
        "\n",
        "    print(f\"IV ATT = {att_iv:.3f}, SE = {se_iv:.3f}, 95% CI = ({ci_iv[0]:.3f}, {ci_iv[1]:.3f})\")\n",
        "    return att_iv, se_iv, ci_iv\n",
        "\n",
        "# Step 3: Instrumental Variables (IV) for ATE\n",
        "def iv_analysis_ate(df, outcome, confounders):\n",
        "    print(\"\\nPerforming IV analysis for ATE...\")\n",
        "    # Check for sufficient variation in treatment and instrument\n",
        "    print(\"Treatment variation check:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "    print(\"Instrument variation check:\")\n",
        "    print(df['instrument'].describe())\n",
        "\n",
        "    if df['treatment'].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in treatment variable.\")\n",
        "    if df['instrument'].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in instrument variable.\")\n",
        "\n",
        "    # Filter confounders to only those with non-NaN values in df\n",
        "    valid_confounders = [c for c in confounders if c in df.columns and df[c].notna().any()]\n",
        "    print(f\"Valid confounders for IV analysis: {valid_confounders}\")\n",
        "\n",
        "    # First stage: Regress treatment (O1) on instrument (P1) and confounders\n",
        "    confounder_terms = [f'Q(\"{c}\")' if ' ' in c else c for c in valid_confounders if not c.startswith('region_')]\n",
        "    formula_first = 'treatment ~ instrument + ' + ' + '.join(confounder_terms)\n",
        "    print(f\"First-stage formula: {formula_first}\")\n",
        "\n",
        "    # Validate formula terms\n",
        "    for term in confounder_terms:\n",
        "        col = term.replace('Q(\"', '').replace('\")', '')\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Column {col} not in DataFrame\")\n",
        "\n",
        "    first_stage = smf.ols(formula_first, data=df).fit()\n",
        "    df['treatment_pred'] = first_stage.predict()\n",
        "\n",
        "    # Check instrument strength (F-statistic on instrument)\n",
        "    f_stat = first_stage.fvalue\n",
        "    print(f\"First-stage F-statistic: {f_stat:.2f}\")\n",
        "    if f_stat < 10:\n",
        "        print(\"Warning: Weak instrument (F < 10). IV results may be unreliable.\")\n",
        "\n",
        "    # Second stage: Regress outcome on predicted treatment and confounders\n",
        "    formula_second = f'{outcome} ~ treatment_pred + ' + ' + '.join(confounder_terms)\n",
        "    print(f\"Second-stage formula: {formula_second}\")\n",
        "    second_stage = smf.ols(formula_second, data=df).fit()\n",
        "\n",
        "    ate_iv = second_stage.params['treatment_pred']\n",
        "    se_iv = second_stage.bse['treatment_pred']\n",
        "    ci_iv = (ate_iv - 1.96 * se_iv, ate_iv + 1.96 * se_iv)\n",
        "\n",
        "    print(f\"IV ATE = {ate_iv:.3f}, SE = {se_iv:.3f}, 95% CI = ({ci_iv[0]:.3f}, {ci_iv[1]:.3f})\")\n",
        "    return ate_iv, se_iv, ci_iv\n",
        "\n",
        "# Step 4: Propensity Score Matching (PSM) for H1Ax=Yes Subgroup\n",
        "def psm_analysis(df, outcome, confounders):\n",
        "    print(\"\\nPerforming PSM analysis for H1Ax=Yes subgroup...\")\n",
        "    # Subset to H1Ax=Yes\n",
        "    sub_df = df[df['H1Ax_Yes'] == 1]\n",
        "    sub_df = sub_df.dropna(axis=1, how='all')\n",
        "    print(f\"H1Ax=Yes subgroup size: {len(sub_df)}\")\n",
        "\n",
        "    if len(sub_df) < 10 or sub_df['treatment'].nunique() < 2:\n",
        "        print(\"Skipping PSM: insufficient data or variation\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Filter confounders to only those with non-NaN values in sub_df\n",
        "    valid_confounders = [c for c in confounders if c in sub_df.columns and sub_df[c].notna().any()]\n",
        "    print(f\"Valid confounders for PSM: {valid_confounders}\")\n",
        "\n",
        "    # Estimate propensity scores\n",
        "    formula_ps = 'treatment ~ ' + ' + '.join([f'Q(\"{c}\")' if ' ' in c else c for c in valid_confounders if not c.startswith('region_')])\n",
        "    ps_model = smf.logit(formula_ps, data=sub_df).fit(disp=0)\n",
        "    sub_df['ps'] = ps_model.predict()\n",
        "\n",
        "    # PSM: Match treated to control units\n",
        "    treated = sub_df[sub_df['treatment'] == 1]\n",
        "    control = sub_df[sub_df['treatment'] == 0]\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=1, radius=0.25)\n",
        "    nn.fit(control[['ps']])\n",
        "    distances, indices = nn.kneighbors(treated[['ps']])\n",
        "\n",
        "    valid_matches = distances.flatten() <= 0.25\n",
        "    matched_treated = treated.index[valid_matches]\n",
        "    matched_control = control.index[indices.flatten()[valid_matches]]\n",
        "\n",
        "    if len(matched_treated) == 0:\n",
        "        print(\"No matches within caliper. Skipping PSM.\")\n",
        "        return None, None, None\n",
        "\n",
        "    matched_df = sub_df.loc[list(matched_treated) + list(matched_control)]\n",
        "    print(f\"Matched dataset size: {len(matched_df)}\")\n",
        "\n",
        "    # Compute ATT using matched sample\n",
        "    att_psm = matched_df[matched_df['treatment'] == 1][outcome].mean() - matched_df[matched_df['treatment'] == 0][outcome].mean()\n",
        "\n",
        "    # Bootstrap for SE\n",
        "    boot_atts = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(len(matched_df), len(matched_df), replace=True)\n",
        "        boot_df = matched_df.iloc[idx]\n",
        "        att_boot = boot_df[boot_df['treatment'] == 1][outcome].mean() - boot_df[boot_df['treatment'] == 0][outcome].mean()\n",
        "        boot_atts.append(att_boot)\n",
        "\n",
        "    se_psm = np.std(boot_atts)\n",
        "    ci_psm = (att_psm - 1.96 * se_psm, att_psm + 1.96 * se_psm)\n",
        "\n",
        "    print(f\"H1Ax=Yes PSM ATT = {att_psm:.3f}, SE = {se_psm:.3f}, 95% CI = ({ci_psm[0]:.3f}, {ci_psm[1]:.3f})\")\n",
        "    return att_psm, se_psm, ci_psm\n",
        "\n",
        "# Step 5: IV Analysis with H4 as Outcome\n",
        "def iv_analysis_h4(df, confounders):\n",
        "    print(\"\\nPerforming IV analysis with H4 as outcome...\")\n",
        "    # Convert H4 to numeric: Much worse=-2, Somewhat worse=-1, About the same=0, Somewhat better=1, Much better=2\n",
        "    df['H4_numeric'] = df['H4'].map({\n",
        "        'Much worse': -2,\n",
        "        'Somewhat worse': -1,\n",
        "        'About the same': 0,\n",
        "        'Somewhat better': 1,\n",
        "        'Much better': 2\n",
        "    })\n",
        "\n",
        "    if df['H4_numeric'].isna().any():\n",
        "        print(\"Warning: Missing or unmapped H4 values. Dropping missing...\")\n",
        "        df = df.dropna(subset=['H4_numeric'])\n",
        "\n",
        "    print(f\"Shape after dropping missing H4_numeric: {df.shape}\")\n",
        "\n",
        "    # Check for sufficient variation in treatment and instrument\n",
        "    print(\"Treatment variation check:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "    print(\"Instrument variation check:\")\n",
        "    print(df['instrument'].describe())\n",
        "\n",
        "    if df['treatment'].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in treatment variable.\")\n",
        "    if df['instrument'].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in instrument variable.\")\n",
        "\n",
        "    # Filter confounders to only those with non-NaN values in df\n",
        "    valid_confounders = [c for c in confounders if c in df.columns and df[c].notna().any()]\n",
        "    print(f\"Valid confounders for IV analysis (H4): {valid_confounders}\")\n",
        "\n",
        "    # First stage: Regress treatment (O1) on instrument (P1) and confounders\n",
        "    confounder_terms = [f'Q(\"{c}\")' if ' ' in c else c for c in valid_confounders if not c.startswith('region_')]\n",
        "    formula_first = 'treatment ~ instrument + ' + ' + '.join(confounder_terms)\n",
        "    print(f\"First-stage formula: {formula_first}\")\n",
        "\n",
        "    # Validate formula terms\n",
        "    for term in confounder_terms:\n",
        "        col = term.replace('Q(\"', '').replace('\")', '')\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Column {col} not in DataFrame\")\n",
        "\n",
        "    first_stage = smf.ols(formula_first, data=df).fit()\n",
        "    df['treatment_pred'] = first_stage.predict()\n",
        "\n",
        "    # Check instrument strength (F-statistic on instrument)\n",
        "    f_stat = first_stage.fvalue\n",
        "    print(f\"First-stage F-statistic: {f_stat:.2f}\")\n",
        "    if f_stat < 10:\n",
        "        print(\"Warning: Weak instrument (F < 10). IV results may be unreliable.\")\n",
        "\n",
        "    # Second stage: Regress H4_numeric on predicted treatment and confounders\n",
        "    formula_second = 'H4_numeric ~ treatment_pred + ' + ' + '.join(confounder_terms)\n",
        "    print(f\"Second-stage formula: {formula_second}\")\n",
        "    second_stage = smf.ols(formula_second, data=df).fit()\n",
        "\n",
        "    att_iv = second_stage.params['treatment_pred']\n",
        "    se_iv = second_stage.bse['treatment_pred']\n",
        "    ci_iv = (att_iv - 1.96 * se_iv, att_iv + 1.96 * se_iv)\n",
        "\n",
        "    print(f\"IV ATT (H4_numeric) = {att_iv:.3f}, SE = {se_iv:.3f}, 95% CI = ({ci_iv[0]:.3f}, {ci_iv[1]:.3f})\")\n",
        "    return att_iv, se_iv, ci_iv\n",
        "\n",
        "# Main Function\n",
        "def main(data):\n",
        "    # Preprocess data\n",
        "    df, outcome, confounder_cols = preprocess_data(data)\n",
        "\n",
        "    # Step 1: IV Analysis for ATT\n",
        "    try:\n",
        "        att_iv, se_iv, ci_iv = iv_analysis_att(df, outcome, confounder_cols)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in IV ATT analysis: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Step 2: IV Analysis for ATE\n",
        "    try:\n",
        "        ate_iv, se_ate, ci_ate = iv_analysis_ate(df, outcome, confounder_cols)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in IV ATE analysis: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Step 3: PSM for H1Ax=Yes Subgroup\n",
        "    try:\n",
        "        att_psm, se_psm, ci_psm = psm_analysis(df, outcome, confounder_cols)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in PSM analysis: {str(e)}\")\n",
        "\n",
        "    # Step 4: IV Analysis with H4 as Outcome\n",
        "    try:\n",
        "        att_h4, se_h4, ci_h4 = iv_analysis_h4(df, confounder_cols)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in IV analysis with H4: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Step 5: Explore Effect with H4 and L1\n",
        "    explore_effect(df, confounder_cols)\n",
        "\n",
        "# Explore Effect with H4 and L1 (already in script, moved to main for clarity)\n",
        "def explore_effect(df, confounder_cols):\n",
        "    print(\"\\nExploring effect with H4 and L1...\")\n",
        "    print(\"H4 by treatment group:\")\n",
        "    print(df.groupby('treatment')['H4'].value_counts(normalize=True))\n",
        "\n",
        "    print(\"L1 (loneliness) by treatment group:\")\n",
        "    print(df.groupby('treatment')['L1'].mean())\n",
        "\n",
        "    # Stratify by H1Ax (pre-existing mental health condition)\n",
        "    print(\"\\nIV ATT by pre-existing mental health condition (H1Ax):\")\n",
        "    for condition in [1, 0]:  # H1Ax_Yes is binary after encoding\n",
        "        sub_df = df[df['H1Ax_Yes'] == condition]\n",
        "        sub_df = sub_df.dropna(axis=1, how='all')  # Drop entirely NaN columns in subgroup\n",
        "        print(f\"\\nH1Ax={'Yes' if condition == 1 else 'No'} subgroup size: {len(sub_df)}\")\n",
        "        print(f\"Treatment value counts in subgroup:\")\n",
        "        print(sub_df['treatment'].value_counts(dropna=False))\n",
        "        print(f\"Instrument summary in subgroup:\")\n",
        "        print(sub_df['instrument'].describe())\n",
        "\n",
        "        if len(sub_df) < 10 or sub_df['treatment'].nunique() < 2 or sub_df['instrument'].nunique() < 2:\n",
        "            print(f\"Skipping H1Ax={'Yes' if condition == 1 else 'No'}: insufficient data or variation (n={len(sub_df)})\")\n",
        "            continue\n",
        "        try:\n",
        "            # Use only confounders present in sub_df\n",
        "            sub_confounders = [c for c in confounder_cols if c in sub_df.columns]\n",
        "            att, se, ci = iv_analysis_att(sub_df, 'QOL', sub_confounders)\n",
        "            print(f\"H1Ax={'Yes' if condition == 1 else 'No'} IV ATT = {att:.3f}, SE = {se:.3f}, 95% CI = ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in H1Ax={'Yes' if condition == 1 else 'No'} subgroup: {str(e)}\")\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    main(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ7FJTACJwYw",
        "outputId": "b5de0e1e-74f2-4075-fd23-9e329a23c357"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing...\n",
            "Initial shape: (1500, 23)\n",
            "O1 value counts:\n",
            "O1\n",
            "No     750\n",
            "Yes    750\n",
            "Name: count, dtype: int64\n",
            "Available columns:\n",
            "['D2', 'region', 'O1', 'O8b', 'P1', 'D1', 'D5', 'D67', 'D4', 'D11', 'D8', 'E3', 'H2', 'H1Ax', 'H4', 'H56', 'S', 'QOL', 'GH', 'L1', 'H3', 'Q2', 'Q1_cat']\n",
            "Treatment value counts:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Instrument (P1) summary:\n",
            "count    1500.000000\n",
            "mean       94.174000\n",
            "std        21.055117\n",
            "min        19.000000\n",
            "25%        79.000000\n",
            "50%        97.500000\n",
            "75%       112.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "Shape after filtering: (1500, 25)\n",
            "Initial confounders: ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
            "Valid confounders: ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
            "Missing values before imputation:\n",
            "QOL           0\n",
            "treatment     0\n",
            "instrument    0\n",
            "D1            0\n",
            "D4            0\n",
            "D5            0\n",
            "D67           0\n",
            "D8            0\n",
            "D11           0\n",
            "H2            0\n",
            "H1Ax          0\n",
            "H56           0\n",
            "region        0\n",
            "dtype: int64\n",
            "Sample data for categorical confounders:\n",
            "      D4           D5         D67    region   H2 H1Ax H56\n",
            "0    Men  High school      Others  Atlantic   No   No  No\n",
            "1    Men      College  Caucasians  Prairies   No   No  No\n",
            "2    Men   University  Caucasians  Atlantic  Yes  Yes  No\n",
            "3    Men  High school  Caucasians   Ontario   No   No  No\n",
            "4  Women   University  Caucasians   Ontario   No   No  No\n",
            "One-hot encoding categorical variables: ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
            "Confounder columns after encoding: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D1', 'D8', 'D11']\n",
            "Continuous variables: ['D1', 'D8', 'D11']\n",
            "Valid continuous variables (non-empty): ['D11']\n",
            "Imputing and standardizing: ['D11']\n",
            "Missing values in categorical confounders: False\n",
            "Shape after dropping entirely NaN columns: (1500, 29)\n",
            "Updated confounder_cols: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11']\n",
            "Data types of confounder columns:\n",
            "D4_Other or did not answer      int64\n",
            "D4_Women                        int64\n",
            "D5_College                      int64\n",
            "D5_High school                  int64\n",
            "D5_University                   int64\n",
            "D67_Others                      int64\n",
            "region_British Columbia         int64\n",
            "region_Ontario                  int64\n",
            "region_Prairies                 int64\n",
            "region_Qu√©bec                   int64\n",
            "H2_Yes                          int64\n",
            "H1Ax_Yes                        int64\n",
            "H56_Yes                         int64\n",
            "D11                           float64\n",
            "dtype: object\n",
            "Missing values after imputation:\n",
            "QOL                           0\n",
            "treatment                     0\n",
            "instrument                    0\n",
            "D4_Other or did not answer    0\n",
            "D4_Women                      0\n",
            "D5_College                    0\n",
            "D5_High school                0\n",
            "D5_University                 0\n",
            "D67_Others                    0\n",
            "region_British Columbia       0\n",
            "region_Ontario                0\n",
            "region_Prairies               0\n",
            "region_Qu√©bec                 0\n",
            "H2_Yes                        0\n",
            "H1Ax_Yes                      0\n",
            "H56_Yes                       0\n",
            "D11                           0\n",
            "dtype: int64\n",
            "Final shape after preprocessing: (1500, 29)\n",
            "\n",
            "Performing IV analysis for ATT...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Instrument variation check:\n",
            "count    1500.000000\n",
            "mean       94.174000\n",
            "std        21.055117\n",
            "min        19.000000\n",
            "25%        79.000000\n",
            "50%        97.500000\n",
            "75%       112.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "Valid confounders for IV analysis: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11']\n",
            "First-stage formula: treatment ~ instrument + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "First-stage F-statistic: 37.55\n",
            "Second-stage formula: QOL ~ treatment_pred + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "IV ATT = -0.028, SE = 0.015, 95% CI = (-0.058, 0.002)\n",
            "\n",
            "Performing IV analysis for ATE...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Instrument variation check:\n",
            "count    1500.000000\n",
            "mean       94.174000\n",
            "std        21.055117\n",
            "min        19.000000\n",
            "25%        79.000000\n",
            "50%        97.500000\n",
            "75%       112.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "Valid confounders for IV analysis: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11']\n",
            "First-stage formula: treatment ~ instrument + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "First-stage F-statistic: 37.55\n",
            "Second-stage formula: QOL ~ treatment_pred + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "IV ATE = -0.028, SE = 0.015, 95% CI = (-0.058, 0.002)\n",
            "\n",
            "Performing PSM analysis for H1Ax=Yes subgroup...\n",
            "H1Ax=Yes subgroup size: 339\n",
            "Valid confounders for PSM: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11']\n",
            "Error in PSM analysis: Singular matrix\n",
            "\n",
            "Performing IV analysis with H4 as outcome...\n",
            "Shape after dropping missing H4_numeric: (1500, 31)\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Instrument variation check:\n",
            "count    1500.000000\n",
            "mean       94.174000\n",
            "std        21.055117\n",
            "min        19.000000\n",
            "25%        79.000000\n",
            "50%        97.500000\n",
            "75%       112.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "Valid confounders for IV analysis (H4): ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11']\n",
            "First-stage formula: treatment ~ instrument + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "First-stage F-statistic: 37.55\n",
            "Second-stage formula: H4_numeric ~ treatment_pred + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "IV ATT (H4_numeric) = -0.250, SE = 0.105, 95% CI = (-0.455, -0.044)\n",
            "\n",
            "Exploring effect with H4 and L1...\n",
            "H4 by treatment group:\n",
            "treatment  H4             \n",
            "0.0        About the same     0.533333\n",
            "           Somewhat worse     0.333333\n",
            "           Somewhat better    0.062667\n",
            "           Much worse         0.053333\n",
            "           Much better        0.017333\n",
            "1.0        About the same     0.418667\n",
            "           Somewhat worse     0.401333\n",
            "           Much worse         0.084000\n",
            "           Somewhat better    0.076000\n",
            "           Much better        0.020000\n",
            "Name: proportion, dtype: float64\n",
            "L1 (loneliness) by treatment group:\n",
            "treatment\n",
            "0.0    5.101333\n",
            "1.0    5.409333\n",
            "Name: L1, dtype: float64\n",
            "\n",
            "IV ATT by pre-existing mental health condition (H1Ax):\n",
            "\n",
            "H1Ax=Yes subgroup size: 339\n",
            "Treatment value counts in subgroup:\n",
            "treatment\n",
            "1.0    214\n",
            "0.0    125\n",
            "Name: count, dtype: int64\n",
            "Instrument summary in subgroup:\n",
            "count    339.000000\n",
            "mean     100.197640\n",
            "std       20.195037\n",
            "min       22.000000\n",
            "25%       89.500000\n",
            "50%      106.000000\n",
            "75%      115.000000\n",
            "max      126.000000\n",
            "Name: instrument, dtype: float64\n",
            "\n",
            "Performing IV analysis for ATT...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "1.0    214\n",
            "0.0    125\n",
            "Name: count, dtype: int64\n",
            "Instrument variation check:\n",
            "count    339.000000\n",
            "mean     100.197640\n",
            "std       20.195037\n",
            "min       22.000000\n",
            "25%       89.500000\n",
            "50%      106.000000\n",
            "75%      115.000000\n",
            "max      126.000000\n",
            "Name: instrument, dtype: float64\n",
            "Valid confounders for IV analysis: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11']\n",
            "First-stage formula: treatment ~ instrument + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "First-stage F-statistic: 9.93\n",
            "Warning: Weak instrument (F < 10). IV results may be unreliable.\n",
            "Second-stage formula: QOL ~ treatment_pred + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "IV ATT = -0.035, SE = 0.052, 95% CI = (-0.137, 0.067)\n",
            "H1Ax=Yes IV ATT = -0.035, SE = 0.052, 95% CI = (-0.137, 0.067)\n",
            "\n",
            "H1Ax=No subgroup size: 1161\n",
            "Treatment value counts in subgroup:\n",
            "treatment\n",
            "0.0    625\n",
            "1.0    536\n",
            "Name: count, dtype: int64\n",
            "Instrument summary in subgroup:\n",
            "count    1161.000000\n",
            "mean       92.415159\n",
            "std        20.984228\n",
            "min        19.000000\n",
            "25%        77.000000\n",
            "50%        95.000000\n",
            "75%       110.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "\n",
            "Performing IV analysis for ATT...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    625\n",
            "1.0    536\n",
            "Name: count, dtype: int64\n",
            "Instrument variation check:\n",
            "count    1161.000000\n",
            "mean       92.415159\n",
            "std        20.984228\n",
            "min        19.000000\n",
            "25%        77.000000\n",
            "50%        95.000000\n",
            "75%       110.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "Valid confounders for IV analysis: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11']\n",
            "First-stage formula: treatment ~ instrument + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "First-stage F-statistic: 29.16\n",
            "Second-stage formula: QOL ~ treatment_pred + Q(\"D4_Other or did not answer\") + D4_Women + D5_College + Q(\"D5_High school\") + D5_University + D67_Others + H2_Yes + H1Ax_Yes + H56_Yes + D11\n",
            "IV ATT = -0.024, SE = 0.014, 95% CI = (-0.052, 0.004)\n",
            "H1Ax=No IV ATT = -0.024, SE = 0.014, 95% CI = (-0.052, 0.004)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimate using LATE"
      ],
      "metadata": {
        "id": "wuxRnqwprLjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def preprocess_data(data):\n",
        "    print(\"Starting preprocessing...\")\n",
        "    df = data.copy()\n",
        "\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "    print(\"O1 value counts:\")\n",
        "    print(df['O1'].value_counts(dropna=False))\n",
        "\n",
        "    print(\"Available columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Define treatment (O1)\n",
        "    df['treatment'] = np.where(df['O1'] == 'Yes', 1,\n",
        "                             np.where(df['O1'] == 'No', 0, np.nan))\n",
        "\n",
        "    # Define instrument (P1) - keep as continuous initially\n",
        "    df['instrument'] = df['P1']\n",
        "\n",
        "    print(\"Treatment value counts:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "\n",
        "    print(\"Instrument (P1) summary:\")\n",
        "    print(df['instrument'].describe())\n",
        "\n",
        "    # Filter to keep only valid treatment and instrument\n",
        "    df = df[df['treatment'].notna() & df['instrument'].notna()]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(f\"Shape after filtering: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows remain after filtering.\")\n",
        "\n",
        "    outcome = 'QOL'\n",
        "    if outcome not in df.columns:\n",
        "        raise ValueError(f\"Outcome column '{outcome}' not found in data.\")\n",
        "\n",
        "    confounders = ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
        "    confounders = [col for col in confounders if col in df.columns]\n",
        "    print(f\"Initial confounders: {confounders}\")\n",
        "\n",
        "    # Exclude confounders with all NaN values initially\n",
        "    valid_confounders = []\n",
        "    for col in confounders:\n",
        "        if df[col].notna().any():\n",
        "            valid_confounders.append(col)\n",
        "        else:\n",
        "            print(f\"Excluding {col}: all values are NaN\")\n",
        "    confounders = valid_confounders\n",
        "    print(f\"Valid confounders: {confounders}\")\n",
        "\n",
        "    print(\"Missing values before imputation:\")\n",
        "    print(df[[outcome, 'treatment', 'instrument'] + confounders].isna().sum())\n",
        "\n",
        "    print(\"Sample data for categorical confounders:\")\n",
        "    categorical_sample = [c for c in ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56'] if c in df.columns]\n",
        "    if categorical_sample:\n",
        "        print(df[categorical_sample].head())\n",
        "\n",
        "    categorical_vars = ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
        "    categorical_vars = [var for var in categorical_vars if var in confounders]\n",
        "    print(f\"One-hot encoding categorical variables: {categorical_vars}\")\n",
        "    if categorical_vars:\n",
        "        df = pd.get_dummies(df, columns=categorical_vars, drop_first=True, dtype=int)\n",
        "\n",
        "    confounder_cols = [col for col in df.columns if any(col.startswith(var + '_') for var in categorical_vars)]\n",
        "    confounder_cols += [var for var in confounders if var not in categorical_vars]\n",
        "    print(f\"Confounder columns after encoding: {confounder_cols}\")\n",
        "\n",
        "    continuous_vars = ['D1', 'D8', 'D11']\n",
        "    continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
        "    print(f\"Continuous variables: {continuous_vars}\")\n",
        "\n",
        "    for var in continuous_vars:\n",
        "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "    valid_continuous_vars = [var for var in continuous_vars if df[var].notna().any()]\n",
        "    print(f\"Valid continuous variables (non-empty): {valid_continuous_vars}\")\n",
        "\n",
        "    if valid_continuous_vars:\n",
        "        print(f\"Imputing and standardizing: {valid_continuous_vars}\")\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        df[valid_continuous_vars] = imputer.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        df[valid_continuous_vars] = scaler.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "    categorical_confounders = [col for col in confounder_cols if col not in continuous_vars]\n",
        "    if categorical_confounders:\n",
        "        missing_cat = df[categorical_confounders].isna().any().any()\n",
        "        print(f\"Missing values in categorical confounders: {missing_cat}\")\n",
        "        if missing_cat:\n",
        "            print(f\"Imputing categorical confounders: {categorical_confounders}\")\n",
        "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "            df[categorical_confounders] = imputer_cat.fit_transform(df[categorical_confounders])\n",
        "\n",
        "    if df[outcome].isna().any():\n",
        "        print(\"Imputing missing QOL values...\")\n",
        "        imputer_outcome = SimpleImputer(strategy='mean')\n",
        "        df[outcome] = imputer_outcome.fit_transform(df[[outcome]])\n",
        "\n",
        "    # Drop columns that are entirely NaN after processing\n",
        "    df = df.dropna(axis=1, how='all')\n",
        "    print(f\"Shape after dropping entirely NaN columns: {df.shape}\")\n",
        "\n",
        "    # Update confounder_cols to only include existing columns\n",
        "    confounder_cols = [col for col in confounder_cols if col in df.columns]\n",
        "    print(f\"Updated confounder_cols: {confounder_cols}\")\n",
        "\n",
        "    print(\"Data types of confounder columns:\")\n",
        "    print(df[confounder_cols].dtypes)\n",
        "\n",
        "    non_numeric_cols = df[confounder_cols].select_dtypes(exclude=['int64', 'float64', 'uint8', 'int32']).columns\n",
        "    if non_numeric_cols.any():\n",
        "        raise ValueError(f\"Non-numeric confounders detected: {non_numeric_cols.tolist()}\")\n",
        "\n",
        "    print(\"Missing values after imputation:\")\n",
        "    final_cols = [outcome, 'treatment', 'instrument'] + confounder_cols\n",
        "    print(df[final_cols].isna().sum())\n",
        "\n",
        "    print(f\"Final shape after preprocessing: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No data remains after preprocessing.\")\n",
        "\n",
        "    return df, outcome, confounder_cols\n",
        "\n",
        "# Step 2: LATE Estimation\n",
        "def late_analysis(df, outcome, instrument_col, treatment_col, threshold=None):\n",
        "    print(f\"\\nPerforming LATE analysis for outcome: {outcome}...\")\n",
        "    # Check for sufficient variation in treatment\n",
        "    print(\"Treatment variation check:\")\n",
        "    print(df[treatment_col].value_counts(dropna=False))\n",
        "\n",
        "    if df[treatment_col].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in treatment variable.\")\n",
        "\n",
        "    # Dichotomize the instrument if continuous\n",
        "    if threshold is None:\n",
        "        threshold = df[instrument_col].median()\n",
        "    print(f\"Using threshold {threshold} to dichotomize instrument...\")\n",
        "    df['Z'] = (df[instrument_col] > threshold).astype(int)\n",
        "\n",
        "    print(\"Instrument (Z) variation check:\")\n",
        "    print(df['Z'].value_counts(dropna=False))\n",
        "\n",
        "    if df['Z'].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in instrument variable after dichotomization.\")\n",
        "\n",
        "    # Estimate LATE using the Wald estimator\n",
        "    # LATE = Cov(Z, Y) / Cov(Z, T)\n",
        "    cov_zy = np.cov(df['Z'], df[outcome])[0, 1]\n",
        "    cov_zt = np.cov(df['Z'], df[treatment_col])[0, 1]\n",
        "\n",
        "    if cov_zt == 0:\n",
        "        raise ValueError(\"Instrument does not affect treatment (Cov(Z, T) = 0).\")\n",
        "\n",
        "    late = cov_zy / cov_zt\n",
        "    print(f\"Cov(Z, Y) = {cov_zy:.4f}, Cov(Z, T) = {cov_zt:.4f}, LATE = {late:.4f}\")\n",
        "\n",
        "    # Bootstrap for standard error and CI\n",
        "    n = len(df)\n",
        "    boot_lates = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        boot_df = df.iloc[idx]\n",
        "        boot_cov_zy = np.cov(boot_df['Z'], boot_df[outcome])[0, 1]\n",
        "        boot_cov_zt = np.cov(boot_df['Z'], boot_df[treatment_col])[0, 1]\n",
        "        if boot_cov_zt == 0:\n",
        "            continue\n",
        "        boot_late = boot_cov_zy / boot_cov_zt\n",
        "        boot_lates.append(boot_late)\n",
        "\n",
        "    se = np.std(boot_lates)\n",
        "    ci = (late - 1.96 * se, late + 1.96 * se)\n",
        "\n",
        "    print(f\"LATE = {late:.3f}, SE = {se:.3f}, 95% CI = ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
        "    return late, se, ci\n",
        "\n",
        "# Step 3: Explore Effect with H4 and L1\n",
        "def explore_effect(df, confounder_cols):\n",
        "    print(\"\\nExploring effect with H4 and L1...\")\n",
        "    print(\"H4 by treatment group:\")\n",
        "    print(df.groupby('treatment')['H4'].value_counts(normalize=True))\n",
        "\n",
        "    print(\"L1 (loneliness) by treatment group:\")\n",
        "    print(df.groupby('treatment')['L1'].mean())\n",
        "\n",
        "    # Stratify by H1Ax (pre-existing mental health condition)\n",
        "    print(\"\\nLATE by pre-existing mental health condition (H1Ax):\")\n",
        "    for condition in [1, 0]:  # H1Ax_Yes is binary after encoding\n",
        "        sub_df = df[df['H1Ax_Yes'] == condition]\n",
        "        sub_df = sub_df.dropna(axis=1, how='all')  # Drop entirely NaN columns in subgroup\n",
        "        print(f\"\\nH1Ax={'Yes' if condition == 1 else 'No'} subgroup size: {len(sub_df)}\")\n",
        "        print(f\"Treatment value counts in subgroup:\")\n",
        "        print(sub_df['treatment'].value_counts(dropna=False))\n",
        "        print(f\"Instrument summary in subgroup:\")\n",
        "        print(sub_df['instrument'].describe())\n",
        "\n",
        "        if len(sub_df) < 10 or sub_df['treatment'].nunique() < 2 or sub_df['instrument'].nunique() < 2:\n",
        "            print(f\"Skipping H1Ax={'Yes' if condition == 1 else 'No'}: insufficient data or variation (n={len(sub_df)})\")\n",
        "            continue\n",
        "        try:\n",
        "            late, se, ci = late_analysis(sub_df, 'QOL', 'instrument', 'treatment')\n",
        "            print(f\"H1Ax={'Yes' if condition == 1 else 'No'} LATE = {late:.3f}, SE = {se:.3f}, 95% CI = ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in H1Ax={'Yes' if condition == 1 else 'No'} subgroup: {str(e)}\")\n",
        "\n",
        "# Main Function\n",
        "def main(data):\n",
        "    # Preprocess data\n",
        "    df, outcome, confounder_cols = preprocess_data(data)\n",
        "\n",
        "    # Step 1: LATE Analysis for QOL\n",
        "    try:\n",
        "        late_qol, se_qol, ci_qol = late_analysis(df, 'QOL', 'instrument', 'treatment')\n",
        "    except Exception as e:\n",
        "        print(f\"Error in LATE analysis for QOL: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Step 2: LATE Analysis for H4_numeric\n",
        "    # Convert H4 to numeric\n",
        "    df['H4_numeric'] = df['H4'].map({\n",
        "        'Much worse': -2,\n",
        "        'Somewhat worse': -1,\n",
        "        'About the same': 0,\n",
        "        'Somewhat better': 1,\n",
        "        'Much better': 2\n",
        "    })\n",
        "\n",
        "    if df['H4_numeric'].isna().any():\n",
        "        print(\"Warning: Missing or unmapped H4 values. Dropping missing...\")\n",
        "        df = df.dropna(subset=['H4_numeric'])\n",
        "\n",
        "    try:\n",
        "        late_h4, se_h4, ci_h4 = late_analysis(df, 'H4_numeric', 'instrument', 'treatment')\n",
        "    except Exception as e:\n",
        "        print(f\"Error in LATE analysis for H4_numeric: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Explore Effect\n",
        "    explore_effect(df, confounder_cols)\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    main(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEl-jmJbJyiD",
        "outputId": "a925c690-2946-4757-ee8a-c4200ab2daea"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing...\n",
            "Initial shape: (1500, 23)\n",
            "O1 value counts:\n",
            "O1\n",
            "No     750\n",
            "Yes    750\n",
            "Name: count, dtype: int64\n",
            "Available columns:\n",
            "['D2', 'region', 'O1', 'O8b', 'P1', 'D1', 'D5', 'D67', 'D4', 'D11', 'D8', 'E3', 'H2', 'H1Ax', 'H4', 'H56', 'S', 'QOL', 'GH', 'L1', 'H3', 'Q2', 'Q1_cat']\n",
            "Treatment value counts:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Instrument (P1) summary:\n",
            "count    1500.000000\n",
            "mean       94.174000\n",
            "std        21.055117\n",
            "min        19.000000\n",
            "25%        79.000000\n",
            "50%        97.500000\n",
            "75%       112.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "Shape after filtering: (1500, 25)\n",
            "Initial confounders: ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
            "Valid confounders: ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
            "Missing values before imputation:\n",
            "QOL           0\n",
            "treatment     0\n",
            "instrument    0\n",
            "D1            0\n",
            "D4            0\n",
            "D5            0\n",
            "D67           0\n",
            "D8            0\n",
            "D11           0\n",
            "H2            0\n",
            "H1Ax          0\n",
            "H56           0\n",
            "region        0\n",
            "dtype: int64\n",
            "Sample data for categorical confounders:\n",
            "      D4           D5         D67    region   H2 H1Ax H56\n",
            "0    Men  High school      Others  Atlantic   No   No  No\n",
            "1    Men      College  Caucasians  Prairies   No   No  No\n",
            "2    Men   University  Caucasians  Atlantic  Yes  Yes  No\n",
            "3    Men  High school  Caucasians   Ontario   No   No  No\n",
            "4  Women   University  Caucasians   Ontario   No   No  No\n",
            "One-hot encoding categorical variables: ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
            "Confounder columns after encoding: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D1', 'D8', 'D11']\n",
            "Continuous variables: ['D1', 'D8', 'D11']\n",
            "Valid continuous variables (non-empty): ['D11']\n",
            "Imputing and standardizing: ['D11']\n",
            "Missing values in categorical confounders: False\n",
            "Shape after dropping entirely NaN columns: (1500, 29)\n",
            "Updated confounder_cols: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D11']\n",
            "Data types of confounder columns:\n",
            "D4_Other or did not answer      int64\n",
            "D4_Women                        int64\n",
            "D5_College                      int64\n",
            "D5_High school                  int64\n",
            "D5_University                   int64\n",
            "D67_Others                      int64\n",
            "region_British Columbia         int64\n",
            "region_Ontario                  int64\n",
            "region_Prairies                 int64\n",
            "region_Qu√©bec                   int64\n",
            "H2_Yes                          int64\n",
            "H1Ax_Yes                        int64\n",
            "H56_Yes                         int64\n",
            "D11                           float64\n",
            "dtype: object\n",
            "Missing values after imputation:\n",
            "QOL                           0\n",
            "treatment                     0\n",
            "instrument                    0\n",
            "D4_Other or did not answer    0\n",
            "D4_Women                      0\n",
            "D5_College                    0\n",
            "D5_High school                0\n",
            "D5_University                 0\n",
            "D67_Others                    0\n",
            "region_British Columbia       0\n",
            "region_Ontario                0\n",
            "region_Prairies               0\n",
            "region_Qu√©bec                 0\n",
            "H2_Yes                        0\n",
            "H1Ax_Yes                      0\n",
            "H56_Yes                       0\n",
            "D11                           0\n",
            "dtype: int64\n",
            "Final shape after preprocessing: (1500, 29)\n",
            "\n",
            "Performing LATE analysis for outcome: QOL...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Using threshold 97.5 to dichotomize instrument...\n",
            "Instrument (Z) variation check:\n",
            "Z\n",
            "0    750\n",
            "1    750\n",
            "Name: count, dtype: int64\n",
            "Cov(Z, Y) = -0.0078, Cov(Z, T) = 0.0941, LATE = -0.0827\n",
            "LATE = -0.083, SE = 0.020, 95% CI = (-0.122, -0.043)\n",
            "\n",
            "Performing LATE analysis for outcome: H4_numeric...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Using threshold 97.5 to dichotomize instrument...\n",
            "Instrument (Z) variation check:\n",
            "Z\n",
            "0    750\n",
            "1    750\n",
            "Name: count, dtype: int64\n",
            "Cov(Z, Y) = -0.0377, Cov(Z, T) = 0.0941, LATE = -0.4007\n",
            "LATE = -0.401, SE = 0.113, 95% CI = (-0.623, -0.179)\n",
            "\n",
            "Exploring effect with H4 and L1...\n",
            "H4 by treatment group:\n",
            "treatment  H4             \n",
            "0.0        About the same     0.533333\n",
            "           Somewhat worse     0.333333\n",
            "           Somewhat better    0.062667\n",
            "           Much worse         0.053333\n",
            "           Much better        0.017333\n",
            "1.0        About the same     0.418667\n",
            "           Somewhat worse     0.401333\n",
            "           Much worse         0.084000\n",
            "           Somewhat better    0.076000\n",
            "           Much better        0.020000\n",
            "Name: proportion, dtype: float64\n",
            "L1 (loneliness) by treatment group:\n",
            "treatment\n",
            "0.0    5.101333\n",
            "1.0    5.409333\n",
            "Name: L1, dtype: float64\n",
            "\n",
            "LATE by pre-existing mental health condition (H1Ax):\n",
            "\n",
            "H1Ax=Yes subgroup size: 339\n",
            "Treatment value counts in subgroup:\n",
            "treatment\n",
            "1.0    214\n",
            "0.0    125\n",
            "Name: count, dtype: int64\n",
            "Instrument summary in subgroup:\n",
            "count    339.000000\n",
            "mean     100.197640\n",
            "std       20.195037\n",
            "min       22.000000\n",
            "25%       89.500000\n",
            "50%      106.000000\n",
            "75%      115.000000\n",
            "max      126.000000\n",
            "Name: instrument, dtype: float64\n",
            "\n",
            "Performing LATE analysis for outcome: QOL...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "1.0    214\n",
            "0.0    125\n",
            "Name: count, dtype: int64\n",
            "Using threshold 106.0 to dichotomize instrument...\n",
            "Instrument (Z) variation check:\n",
            "Z\n",
            "0    175\n",
            "1    164\n",
            "Name: count, dtype: int64\n",
            "Cov(Z, Y) = -0.0024, Cov(Z, T) = 0.0842, LATE = -0.0291\n",
            "LATE = -0.029, SE = 0.053, 95% CI = (-0.134, 0.076)\n",
            "H1Ax=Yes LATE = -0.029, SE = 0.053, 95% CI = (-0.134, 0.076)\n",
            "\n",
            "H1Ax=No subgroup size: 1161\n",
            "Treatment value counts in subgroup:\n",
            "treatment\n",
            "0.0    625\n",
            "1.0    536\n",
            "Name: count, dtype: int64\n",
            "Instrument summary in subgroup:\n",
            "count    1161.000000\n",
            "mean       92.415159\n",
            "std        20.984228\n",
            "min        19.000000\n",
            "25%        77.000000\n",
            "50%        95.000000\n",
            "75%       110.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "\n",
            "Performing LATE analysis for outcome: QOL...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    625\n",
            "1.0    536\n",
            "Name: count, dtype: int64\n",
            "Using threshold 95.0 to dichotomize instrument...\n",
            "Instrument (Z) variation check:\n",
            "Z\n",
            "0    586\n",
            "1    575\n",
            "Name: count, dtype: int64\n",
            "Cov(Z, Y) = -0.0025, Cov(Z, T) = 0.0970, LATE = -0.0253\n",
            "LATE = -0.025, SE = 0.016, 95% CI = (-0.057, 0.007)\n",
            "H1Ax=No LATE = -0.025, SE = 0.016, 95% CI = (-0.057, 0.007)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Step 1: Data Preprocessing (Including Observed Confounders)\n",
        "def preprocess_data(data):\n",
        "    print(\"Starting preprocessing...\")\n",
        "    df = data.copy()\n",
        "\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "    print(\"O1 value counts:\")\n",
        "    print(df['O1'].value_counts(dropna=False))\n",
        "\n",
        "    print(\"Available columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Define treatment (O1)\n",
        "    df['treatment'] = np.where(df['O1'] == 'Yes', 1,\n",
        "                             np.where(df['O1'] == 'No', 0, np.nan))\n",
        "\n",
        "    # Define instrument (P1)\n",
        "    df['instrument'] = df['P1']\n",
        "\n",
        "    print(\"Treatment value counts:\")\n",
        "    print(df['treatment'].value_counts(dropna=False))\n",
        "\n",
        "    print(\"Instrument (P1) summary:\")\n",
        "    print(df['instrument'].describe())\n",
        "\n",
        "    # Filter to keep only valid treatment and instrument\n",
        "    df = df[df['treatment'].notna() & df['instrument'].notna()]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(f\"Shape after filtering: {df.shape}\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows remain after filtering.\")\n",
        "\n",
        "    # Ensure QOL and H4 are present\n",
        "    if 'QOL' not in df.columns:\n",
        "        raise ValueError(\"Outcome column 'QOL' not found in data.\")\n",
        "    if 'H4' not in df.columns:\n",
        "        raise ValueError(\"Outcome column 'H4' not found in data.\")\n",
        "\n",
        "    # Convert H4 to numeric\n",
        "    df['H4_numeric'] = df['H4'].map({\n",
        "        'Much worse': -2,\n",
        "        'Somewhat worse': -1,\n",
        "        'About the same': 0,\n",
        "        'Somewhat better': 1,\n",
        "        'Much better': 2\n",
        "    })\n",
        "\n",
        "    if df['H4_numeric'].isna().any():\n",
        "        print(\"Warning: Missing or unmapped H4 values. Dropping missing...\")\n",
        "        df = df.dropna(subset=['H4_numeric'])\n",
        "\n",
        "    # Handle confounders\n",
        "    confounders = ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
        "    confounders = [col for col in confounders if col in df.columns]\n",
        "    print(f\"Initial confounders: {confounders}\")\n",
        "\n",
        "    # Exclude confounders with all NaN values\n",
        "    valid_confounders = []\n",
        "    for col in confounders:\n",
        "        if df[col].notna().any():\n",
        "            valid_confounders.append(col)\n",
        "        else:\n",
        "            print(f\"Excluding {col}: all values are NaN\")\n",
        "    confounders = valid_confounders\n",
        "    print(f\"Valid confounders: {confounders}\")\n",
        "\n",
        "    # One-hot encode categorical confounders\n",
        "    categorical_vars = ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
        "    categorical_vars = [var for var in categorical_vars if var in confounders]\n",
        "    print(f\"One-hot encoding categorical variables: {categorical_vars}\")\n",
        "    if categorical_vars:\n",
        "        df = pd.get_dummies(df, columns=categorical_vars, drop_first=True, dtype=int)\n",
        "\n",
        "    confounder_cols = [col for col in df.columns if any(col.startswith(var + '_') for var in categorical_vars)]\n",
        "    continuous_vars = ['D1', 'D8', 'D11']\n",
        "    continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
        "    confounder_cols += [var for var in continuous_vars if var in df.columns]\n",
        "    print(f\"Confounder columns after encoding: {confounder_cols}\")\n",
        "\n",
        "    for var in continuous_vars:\n",
        "        if var in df.columns:\n",
        "            df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "    valid_continuous_vars = [var for var in continuous_vars if var in df.columns and df[var].notna().any()]\n",
        "    print(f\"Valid continuous variables (non-empty): {valid_continuous_vars}\")\n",
        "\n",
        "    if valid_continuous_vars:\n",
        "        print(f\"Imputing and standardizing: {valid_continuous_vars}\")\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        df[valid_continuous_vars] = imputer.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        df[valid_continuous_vars] = scaler.fit_transform(df[valid_continuous_vars])\n",
        "\n",
        "    categorical_confounders = [col for col in confounder_cols if col not in continuous_vars]\n",
        "    if categorical_confounders:\n",
        "        missing_cat = df[categorical_confounders].isna().any().any()\n",
        "        print(f\"Missing values in categorical confounders: {missing_cat}\")\n",
        "        if missing_cat:\n",
        "            print(f\"Imputing categorical confounders: {categorical_confounders}\")\n",
        "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "            df[categorical_confounders] = imputer_cat.fit_transform(df[categorical_confounders])\n",
        "\n",
        "    print(f\"Final shape after preprocessing: {df.shape}\")\n",
        "    return df, confounder_cols\n",
        "\n",
        "# Step 2: LATE Estimation\n",
        "def late_analysis(df, outcome, instrument_col, treatment_col, threshold=None):\n",
        "    print(f\"\\nPerforming LATE analysis for outcome: {outcome}...\")\n",
        "    # Check for sufficient variation in treatment\n",
        "    print(\"Treatment variation check:\")\n",
        "    print(df[treatment_col].value_counts(dropna=False))\n",
        "\n",
        "    if df[treatment_col].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in treatment variable.\")\n",
        "\n",
        "    # Dichotomize the instrument if continuous\n",
        "    if threshold is None:\n",
        "        threshold = df[instrument_col].median()\n",
        "    print(f\"Using threshold {threshold} to dichotomize instrument...\")\n",
        "    df['Z'] = (df[instrument_col] > threshold).astype(int)\n",
        "\n",
        "    print(\"Instrument (Z) variation check:\")\n",
        "    print(df['Z'].value_counts(dropna=False))\n",
        "\n",
        "    if df['Z'].nunique() < 2:\n",
        "        raise ValueError(\"Insufficient variation in instrument variable after dichotomization.\")\n",
        "\n",
        "    # Estimate LATE using the Wald estimator\n",
        "    cov_zy = np.cov(df['Z'], df[outcome])[0, 1]\n",
        "    cov_zt = np.cov(df['Z'], df[treatment_col])[0, 1]\n",
        "\n",
        "    if cov_zt == 0:\n",
        "        raise ValueError(\"Instrument does not affect treatment (Cov(Z, T) = 0).\")\n",
        "\n",
        "    late = cov_zy / cov_zt\n",
        "    print(f\"Cov(Z, Y) = {cov_zy:.4f}, Cov(Z, T) = {cov_zt:.4f}, LATE = {late:.4f}\")\n",
        "\n",
        "    # Bootstrap for standard error and CI\n",
        "    n = len(df)\n",
        "    boot_lates = []\n",
        "    for _ in range(200):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        boot_df = df.iloc[idx]\n",
        "        boot_cov_zy = np.cov(boot_df['Z'], boot_df[outcome])[0, 1]\n",
        "        boot_cov_zt = np.cov(boot_df['Z'], boot_df[treatment_col])[0, 1]\n",
        "        if boot_cov_zt == 0:\n",
        "            continue\n",
        "        boot_late = boot_cov_zy / boot_cov_zt\n",
        "        boot_lates.append(boot_late)\n",
        "\n",
        "    se = np.std(boot_lates)\n",
        "    ci = (late - 1.96 * se, late + 1.96 * se)\n",
        "\n",
        "    print(f\"LATE = {late:.3f}, SE = {se:.3f}, 95% CI = ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
        "    return late, se, ci, threshold\n",
        "\n",
        "# Step 3: Calculate Partial R^2 for Observed Confounders\n",
        "def calculate_partial_r2(df, confounder_cols, target):\n",
        "    r2_values = []\n",
        "    for col in confounder_cols:\n",
        "        # Prepare data\n",
        "        X = df[[col]].dropna()\n",
        "        y = df.loc[X.index, target]\n",
        "        if len(X) == 0 or y.isna().all():\n",
        "            continue\n",
        "        # Fit linear regression\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "        # Calculate R^2\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        r2_values.append((col, r2))\n",
        "    return r2_values\n",
        "\n",
        "# Step 4: Sensitivity Analysis and Austin Plot with Adjusted Legends\n",
        "def sensitivity_analysis_austen_plot(df, outcome, instrument_col, treatment_col, confounder_cols, threshold, outcome_name, late_orig, se_orig):\n",
        "    print(f\"\\nGenerating sensitivity analysis and Austin plot for outcome: {outcome} (Simulating unmeasured confounder U)...\")\n",
        "\n",
        "    # Dichotomize the instrument\n",
        "    df['Z'] = (df[instrument_col] > threshold).astype(int)\n",
        "\n",
        "    # Calculate partial R^2 for observed confounders\n",
        "    r2_z = calculate_partial_r2(df, confounder_cols, 'Z')\n",
        "    r2_y = calculate_partial_r2(df, confounder_cols, outcome)\n",
        "\n",
        "    # Create a dictionary mapping confounder to (R^2_UZ, R^2_UY)\n",
        "    confounder_r2 = {}\n",
        "    for (conf_z, r2_uz), (conf_y, r2_uy) in zip(r2_z, r2_y):\n",
        "        if conf_z == conf_y:\n",
        "            confounder_r2[conf_z] = (r2_uz, r2_uy)\n",
        "\n",
        "    print(\"\\nPartial R^2 of observed confounders:\")\n",
        "    for conf, (r2_uz, r2_uy) in confounder_r2.items():\n",
        "        print(f\"{conf}: R^2_UZ = {r2_uz:.3f}, R^2_UY = {r2_uy:.3f}\")\n",
        "\n",
        "    # Generate distinct colors for each confounder\n",
        "    colors = list(mcolors.TABLEAU_COLORS.values())[:len(confounder_r2)]\n",
        "    if len(colors) < len(confounder_r2):\n",
        "        colors.extend(list(mcolors.CSS4_COLORS.values())[:len(confounder_r2) - len(colors)])\n",
        "\n",
        "    # Generate hypothetical unmeasured confounder U\n",
        "    df['U'] = np.random.normal(0, 1, len(df))\n",
        "\n",
        "    # Vary R^2 values for unmeasured confounding\n",
        "    r2_values = np.linspace(0.0, 0.5, 21)  # 0.0 to 0.5 in 21 steps\n",
        "    adjusted_lates = np.zeros((len(r2_values), len(r2_values)))\n",
        "\n",
        "    # Standard deviation of the outcome for scaling\n",
        "    sd_y = np.std(df[outcome])\n",
        "\n",
        "    for i, r2_uz in enumerate(r2_values):\n",
        "        for j, r2_uy in enumerate(r2_values):\n",
        "            # Adjust Z and Y for unmeasured confounding\n",
        "            df['Z_adj'] = np.sqrt(1 - r2_uz) * df['Z'] + np.sqrt(r2_uz) * df['U']\n",
        "            df['Y_adj'] = df[outcome] + np.sqrt(r2_uy) * df['U'] * sd_y\n",
        "\n",
        "            # Recalculate LATE\n",
        "            cov_zy_adj = np.cov(df['Z_adj'], df['Y_adj'])[0, 1]\n",
        "            cov_zt_adj = np.cov(df['Z_adj'], df[treatment_col])[0, 1]\n",
        "            if cov_zt_adj == 0:\n",
        "                adjusted_lates[i, j] = np.nan\n",
        "            else:\n",
        "                adjusted_lates[i, j] = cov_zy_adj / cov_zt_adj\n",
        "\n",
        "    # Generate Austin Plot with Adjusted Legends\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Contour plot of adjusted LATE values\n",
        "    contour = plt.contourf(r2_values, r2_values, adjusted_lates, levels=20, cmap='viridis')\n",
        "    plt.colorbar(contour, label=f'Adjusted LATE for {outcome_name}', pad=0.1, fraction=0.046)\n",
        "\n",
        "    # Contour for LATE = 0 (null effect)\n",
        "    plt.contour(r2_values, r2_values, adjusted_lates, levels=[0], colors='red', linestyles='dashed', linewidths=2)\n",
        "\n",
        "    # Contours for significance bounds (95% CI)\n",
        "    ci_lower = late_orig - 1.96 * se_orig\n",
        "    ci_upper = late_orig + 1.96 * se_orig\n",
        "    plt.contour(r2_values, r2_values, adjusted_lates, levels=[ci_lower, ci_upper], colors='white', linestyles='dotted', linewidths=2)\n",
        "\n",
        "    # Plot observed confounders as scatter points with distinct colors and markers\n",
        "    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', 'h', '*']  # Different markers for each confounder\n",
        "    for idx, (conf, (r2_uz, r2_uy)) in enumerate(confounder_r2.items()):\n",
        "        plt.scatter(r2_uz, r2_uy, label=conf, color=colors[idx % len(colors)],\n",
        "                    marker=markers[idx % len(markers)], s=150, alpha=0.8, edgecolors='black')\n",
        "\n",
        "    # Add annotations with larger, bold font\n",
        "    plt.text(0.02, 0.48, f'Original LATE: {late_orig:.3f}', color='white', fontsize=12, fontweight='bold',\n",
        "             bbox=dict(facecolor='black', alpha=0.5))\n",
        "    plt.text(0.02, 0.44, f'CI: ({ci_lower:.3f}, {ci_upper:.3f})', color='white', fontsize=12, fontweight='bold',\n",
        "             bbox=dict(facecolor='black', alpha=0.5))\n",
        "\n",
        "    # Set axis labels and title with larger, bold font\n",
        "    plt.xlabel('$R^2_{UZ}$ (Association with Instrument)', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('$R^2_{UY}$ (Association with Outcome)', fontsize=14, fontweight='bold')\n",
        "    plt.title(f'Austin Plot for {outcome_name}: Sensitivity to Unmeasured Confounding\\n(Points Represent Observed Confounders)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    # Customize legend to prevent overlap\n",
        "    # Position legend below the plot with multiple columns if needed\n",
        "    num_confounders = len(confounder_r2)\n",
        "    num_cols = min(3, max(1, num_confounders // 4 + 1))  # Adjust number of columns based on number of confounders\n",
        "    plt.legend(title=\"Observed Confounders\", fontsize=10, title_fontsize=12,\n",
        "               loc='upper center', bbox_to_anchor=(0.5, -0.15),\n",
        "               ncol=num_cols, frameon=True, edgecolor='black', markerscale=1.5)\n",
        "\n",
        "    # Adjust layout to prevent overlap\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.25)  # Add extra space at the bottom for the legend\n",
        "    plt.savefig(f'austen_plot_{outcome_name.lower()}_with_observed_confounders.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return adjusted_lates\n",
        "\n",
        "# Main Function\n",
        "def main(data):\n",
        "    # Preprocess data\n",
        "    df, confounder_cols = preprocess_data(data)\n",
        "\n",
        "    # Step 1: LATE Analysis for QOL\n",
        "    try:\n",
        "        late_qol, se_qol, ci_qol, threshold_qol = late_analysis(df, 'QOL', 'instrument', 'treatment')\n",
        "    except Exception as e:\n",
        "        print(f\"Error in LATE analysis for QOL: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Step 2: LATE Analysis for H4_numeric\n",
        "    try:\n",
        "        late_h4, se_h4, ci_h4, threshold_h4 = late_analysis(df, 'H4_numeric', 'instrument', 'treatment')\n",
        "    except Exception as e:\n",
        "        print(f\"Error in LATE analysis for H4_numeric: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Generate Austin Plots with Observed Confounders\n",
        "    try:\n",
        "        # For QOL\n",
        "        adjusted_lates_qol = sensitivity_analysis_austen_plot(df, 'QOL', 'instrument', 'treatment', confounder_cols, threshold_qol, 'QOL', late_qol, se_qol)\n",
        "        # For H4_numeric\n",
        "        adjusted_lates_h4 = sensitivity_analysis_austen_plot(df, 'H4_numeric', 'instrument', 'treatment', confounder_cols, threshold_h4, 'H4_numeric', late_h4, se_h4)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generating Austin plots: {str(e)}\")\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'data' is your DataFrame\n",
        "    # Replace 'data' with your actual DataFrame variable\n",
        "    main(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQYEfXRsbWv3",
        "outputId": "0ef64c08-7dfe-4065-f311-45de646b08a7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing...\n",
            "Initial shape: (1500, 23)\n",
            "O1 value counts:\n",
            "O1\n",
            "No     750\n",
            "Yes    750\n",
            "Name: count, dtype: int64\n",
            "Available columns:\n",
            "['D2', 'region', 'O1', 'O8b', 'P1', 'D1', 'D5', 'D67', 'D4', 'D11', 'D8', 'E3', 'H2', 'H1Ax', 'H4', 'H56', 'S', 'QOL', 'GH', 'L1', 'H3', 'Q2', 'Q1_cat']\n",
            "Treatment value counts:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Instrument (P1) summary:\n",
            "count    1500.000000\n",
            "mean       94.174000\n",
            "std        21.055117\n",
            "min        19.000000\n",
            "25%        79.000000\n",
            "50%        97.500000\n",
            "75%       112.000000\n",
            "max       126.000000\n",
            "Name: instrument, dtype: float64\n",
            "Shape after filtering: (1500, 25)\n",
            "Initial confounders: ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
            "Valid confounders: ['D1', 'D4', 'D5', 'D67', 'D8', 'D11', 'H2', 'H1Ax', 'H56', 'region']\n",
            "One-hot encoding categorical variables: ['D4', 'D5', 'D67', 'region', 'H2', 'H1Ax', 'H56']\n",
            "Confounder columns after encoding: ['D4_Other or did not answer', 'D4_Women', 'D5_College', 'D5_High school', 'D5_University', 'D67_Others', 'region_British Columbia', 'region_Ontario', 'region_Prairies', 'region_Qu√©bec', 'H2_Yes', 'H1Ax_Yes', 'H56_Yes', 'D1', 'D8', 'D11']\n",
            "Valid continuous variables (non-empty): ['D11']\n",
            "Imputing and standardizing: ['D11']\n",
            "Missing values in categorical confounders: False\n",
            "Final shape after preprocessing: (1500, 32)\n",
            "\n",
            "Performing LATE analysis for outcome: QOL...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Using threshold 97.5 to dichotomize instrument...\n",
            "Instrument (Z) variation check:\n",
            "Z\n",
            "0    750\n",
            "1    750\n",
            "Name: count, dtype: int64\n",
            "Cov(Z, Y) = -0.0078, Cov(Z, T) = 0.0941, LATE = -0.0827\n",
            "LATE = -0.083, SE = 0.018, 95% CI = (-0.118, -0.048)\n",
            "\n",
            "Performing LATE analysis for outcome: H4_numeric...\n",
            "Treatment variation check:\n",
            "treatment\n",
            "0.0    750\n",
            "1.0    750\n",
            "Name: count, dtype: int64\n",
            "Using threshold 97.5 to dichotomize instrument...\n",
            "Instrument (Z) variation check:\n",
            "Z\n",
            "0    750\n",
            "1    750\n",
            "Name: count, dtype: int64\n",
            "Cov(Z, Y) = -0.0377, Cov(Z, T) = 0.0941, LATE = -0.4007\n",
            "LATE = -0.401, SE = 0.115, 95% CI = (-0.627, -0.175)\n",
            "\n",
            "Generating sensitivity analysis and Austin plot for outcome: QOL (Simulating unmeasured confounder U)...\n",
            "\n",
            "Partial R^2 of observed confounders:\n",
            "D4_Other or did not answer: R^2_UZ = 0.000, R^2_UY = 0.000\n",
            "D4_Women: R^2_UZ = 0.037, R^2_UY = 0.005\n",
            "D5_College: R^2_UZ = 0.005, R^2_UY = 0.002\n",
            "D5_High school: R^2_UZ = 0.004, R^2_UY = 0.002\n",
            "D5_University: R^2_UZ = 0.016, R^2_UY = 0.015\n",
            "D67_Others: R^2_UZ = 0.027, R^2_UY = 0.000\n",
            "region_British Columbia: R^2_UZ = 0.000, R^2_UY = 0.000\n",
            "region_Ontario: R^2_UZ = 0.001, R^2_UY = 0.001\n",
            "region_Prairies: R^2_UZ = 0.001, R^2_UY = 0.001\n",
            "region_Qu√©bec: R^2_UZ = 0.001, R^2_UY = 0.006\n",
            "H2_Yes: R^2_UZ = 0.005, R^2_UY = 0.211\n",
            "H1Ax_Yes: R^2_UZ = 0.024, R^2_UY = 0.173\n",
            "H56_Yes: R^2_UZ = 0.000, R^2_UY = 0.002\n",
            "D11: R^2_UZ = 0.000, R^2_UY = 0.001\n",
            "\n",
            "Generating sensitivity analysis and Austin plot for outcome: H4_numeric (Simulating unmeasured confounder U)...\n",
            "\n",
            "Partial R^2 of observed confounders:\n",
            "D4_Other or did not answer: R^2_UZ = 0.000, R^2_UY = 0.000\n",
            "D4_Women: R^2_UZ = 0.037, R^2_UY = 0.008\n",
            "D5_College: R^2_UZ = 0.005, R^2_UY = 0.001\n",
            "D5_High school: R^2_UZ = 0.004, R^2_UY = 0.001\n",
            "D5_University: R^2_UZ = 0.016, R^2_UY = 0.000\n",
            "D67_Others: R^2_UZ = 0.027, R^2_UY = 0.001\n",
            "region_British Columbia: R^2_UZ = 0.000, R^2_UY = 0.000\n",
            "region_Ontario: R^2_UZ = 0.001, R^2_UY = 0.002\n",
            "region_Prairies: R^2_UZ = 0.001, R^2_UY = 0.000\n",
            "region_Qu√©bec: R^2_UZ = 0.001, R^2_UY = 0.001\n",
            "H2_Yes: R^2_UZ = 0.005, R^2_UY = 0.001\n",
            "H1Ax_Yes: R^2_UZ = 0.024, R^2_UY = 0.041\n",
            "H56_Yes: R^2_UZ = 0.000, R^2_UY = 0.000\n",
            "D11: R^2_UZ = 0.000, R^2_UY = 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9nVFFBmfiLP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}